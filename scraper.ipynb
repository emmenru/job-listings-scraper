{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaf65a-cd7c-4c50-aa5c-450e75643b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages \n",
    "#!pip install beautifulsoup4 lxml selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3c9e26e-5774-4eff-b3df-22d752103b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import time\n",
    "import requests\n",
    "from csv import writer\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree as et\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0fba3dd-183d-4e56-bb44-0d609c741104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define job and location search keywords\n",
    "job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']\n",
    "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']\n",
    "\n",
    "# define base and pagination URLs\n",
    "base_url = 'https://www.indeed.com'\n",
    "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
    "\n",
    "# check also Sweden and France \n",
    "# Just pick the 3 largest cities in each country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a2aaf-6755-4452-9d3c-16de2b6545b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcde12ed-f6f2-4e9b-b707-f22f81a8f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get DOM from given URL\n",
    "def get_dom(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Ensure page loads\n",
    "    page_content = driver.page_source\n",
    "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    dom = et.HTML(str(product_soup))\n",
    "    return dom\n",
    "\n",
    "# Function to extract job link\n",
    "def get_job_link(job):\n",
    "    try:\n",
    "        return job.xpath('./descendant::h2/a/@href')[0]\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "# Function to extract job description\n",
    "def get_job_desc(job_link):\n",
    "    job_dom = get_dom(job_link)\n",
    "    try:\n",
    "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
    "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "def get_skills(job_link):\n",
    "    job_dom = get_dom(job_link)\n",
    "    try:\n",
    "        # Use a refined XPath to get all relevant text within the profile insights\n",
    "        skills = job_dom.xpath('//div[contains(@class, \"profile-insights\")]//text()')\n",
    "        return \" \".join(skills).strip() if skills else 'Not available'\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "# functions to extract job title\n",
    "def get_job_title(job):\n",
    "   try:\n",
    "       job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
    "   except Exception as e:\n",
    "       job_title = 'Not available'\n",
    "   return job_title\n",
    "\n",
    "# functions to extract the company name\n",
    "def get_company_name(job):\n",
    "   try:\n",
    "       company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
    "       #company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_name = 'Not available'\n",
    "   return company_name\n",
    "\n",
    "# functions to extract the company location\n",
    "def get_company_location(job):\n",
    "   try:\n",
    "       company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
    "       #company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_location = 'Not available'\n",
    "   return company_location\n",
    "\n",
    "# functions to extract salary information\n",
    "'''\n",
    "def get_salary(job):\n",
    "   try:\n",
    "       salary = job.xpath('./descendant::span[@class=\"estimated-salary\"]/span/text()')\n",
    "   except Exception as e:\n",
    "       salary = 'Not available'\n",
    "   if len(salary) == 0:\n",
    "       try:\n",
    "           salary = job.xpath('./descendant::div[@class=\"metadata salary-snippet-container\"]/div/text()')[0]\n",
    "       except Exception as e:\n",
    "           salary = 'Not available'\n",
    "   else:\n",
    "       salary = salary[0]\n",
    "   return salary\n",
    "'''\n",
    "\n",
    "def get_salary(job_link):\n",
    "    job_dom = get_dom(job_link)\n",
    "    try:\n",
    "        # Use the provided XPath to get the salary text\n",
    "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
    "        return \" \".join(salary).strip() if salary else 'Not available'\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "        \n",
    "# functions to extract job type\n",
    "def get_job_type(job):\n",
    "   try:\n",
    "       job_type = job.xpath('./descendant::div[@class=\"metadata\"]/div/text()')[0]\n",
    "   except Exception as e:\n",
    "       job_type = 'Not available'\n",
    "   return job_type\n",
    "\n",
    "def get_total_pages(job_keyword, location_keyword):\n",
    "\n",
    "    # URL of Indeed job search\n",
    "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the element containing the job count to appear\n",
    "        job_count_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
    "        )\n",
    "        \n",
    "        # Extract the text from the element\n",
    "        job_count_text = job_count_element.text\n",
    "        print(f\"Job count text: {job_count_text}\")\n",
    "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
    "\n",
    "        # Each page shows 15 jobs\n",
    "        jobs_per_page = 15\n",
    "\n",
    "        # Calculate the total number of pages\n",
    "        total_pages = math.ceil(job_count / jobs_per_page)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "        return total_pages\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting job count: {e}\")\n",
    "        return 0  # Return 0 if there's an error\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        #driver.quit()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dabfd285-f1bd-49ae-97be-de319ac6b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_total_pages(job_keyword, location_keyword):\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inspect content of a Python file \n",
    "#import inspect\n",
    "#lines = inspect.getsource(get_total_pages)\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc8a2a-339f-4938-baf3-5133bdd6de9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: Data Scientist in New York\n",
      "Job count text: 300+ jobs\n",
      "Total pages: 20\n",
      "Total pages found: 20\n",
      "Fetching Page number: 1\n",
      "Jobs found on page 1: 15\n",
      "1 https://www.indeed.com/rc/clk?jk=d1b96df5f19292fd&bb=uFHC_idxkOdMhSd7jLvpbdmVqkVMRizgaSppHa9RnBr5gnqggwT93h0dkyJfb49X6Z38zSWR6IU_au0MIH0dA2STlyJXalgYBNnd73aMRl6PPOcqUyRb5Q%3D%3D&xkcb=SoCu67M37HRhBYygNx0LbzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=99eba59d82a9848e&bb=uFHC_idxkOdMhSd7jLvpbdhn7tsseqUWJpd0uRnCwxvyFHxwr2_SZq8Oiv73i9xgXGeIFo2L4AvwsqqRyCpZTVUo7IdYw52m9iAX7or8z1HnkWUwrUbAIvxAkAJ83tTh&xkcb=SoAa67M37HRhBYygNx0KbzkdCdPP&fccid=7d7b563c6a3a9653&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=be8ea87cdf172e4a&bb=uFHC_idxkOdMhSd7jLvpbQwud5KikuDEe9aAleb4Sbrz0uB5BZATDRCb2Pjap6FijFBpvMD-T1GeAN4uNVXBMeuHUVR3a4T1Rn1TofLABUKS5zV9L_5l9g%3D%3D&xkcb=SoCH67M37HRhBYygNx0JbzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=0d47ada5c09e40f9&bb=uFHC_idxkOdMhSd7jLvpbchWD--rU99SyqviGfVnqKtFkxcESA2Tkl1LMOqS493lP1uL5LiVH-uxy0TgITu2qSWc1RNPMTSQEetu5C3uVfd9ESYcKSXMFA%3D%3D&xkcb=SoAz67M37HRhBYygNx0IbzkdCdPP&fccid=734cb5a01ee60f80&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=5947e369ab469f87&bb=uFHC_idxkOdMhSd7jLvpbT2cs7jSM_qq5eWiMldDJ6MwTBkO-aGUKUFU4iub-UOpXcr5crEt25xFbYTfMFnH0rmJnbKTB2OzN5hhSvIaGSyapDWPO9MNDcaPQXZMerBz&xkcb=SoC967M37HRhBYygNx0PbzkdCdPP&fccid=06bff29399a4f5bb&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=72aeef5ff8ec1f8c&bb=uFHC_idxkOdMhSd7jLvpbTm83PHoEApeXWXp8YtNkbCJ3eq9yjyyXvjg0VeOoxChylscla_cbaMiD1ZTTSOPEz5F0yl5SlvqFy4WCxsiVOytzKFe_oevcw%3D%3D&xkcb=SoAJ67M37HRhBYygNx0ObzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=dc3122ee92633a65&bb=uFHC_idxkOdMhSd7jLvpbWTn14AT-K5pLQdOC0Qgpjrf-YfFQt9Nt08YZzgzuzDEN3CtS5rUL5TzTAqjyLg7AaBE0vM7F-_6CJS_INUXsSAVG4Vi7GLP5Q%3D%3D&xkcb=SoCU67M37HRhBYygNx0NbzkdCdPP&fccid=fe2d21eef233e94a&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=1e01be32ee06a2f3&bb=uFHC_idxkOdMhSd7jLvpbWRiHotGB-KcSz40Iwdcz9aqXVs2XFScv1okxua06ZjzBiJzvapeEp1WjOMbyepHH3ijJLYcqYSosIZ-l8aawUR0vYxfUXbVTcg7HRjSKnbp&xkcb=SoAg67M37HRhBYygNx0MbzkdCdPP&fccid=38275e08b2d0f536&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=b6706b401082dae9&bb=uFHC_idxkOdMhSd7jLvpbfmnnvLwPCcQuWPdzuHvRB6PSqS_3_7CCXJmu6C45utZWZMTdupauAtS_eNWy56YLhuRQfqS2jYCqxhdgmtGyPEwmkCfCksIAA%3D%3D&xkcb=SoDJ67M37HRhBYygNx0DbzkdCdPP&fccid=be3b11aa573faee7&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=2baab3a6e1f0038f&bb=uFHC_idxkOdMhSd7jLvpbeQvmgyCKss2pkXPl8aj49RTM2_Cx88nPziwFPU9kUTv_5XINgK_BkWn60dEjyQHSr3TDHbtosa-QE76xjD4IQZS_Gjg_kVelqt-Ph1H-Dt0&xkcb=SoB967M37HRhBYygNx0CbzkdCdPP&fccid=92278e592a11e3a5&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=0e8c12605caebe99&bb=uFHC_idxkOdMhSd7jLvpbbhSZFgAurfo2MMMtwlzpcjKOKZSMI6nmL47P-LCAN_KaXts5PTRTXQDolvMU7bNTBCu3aL-zTPk-MfMAYts0f5dGU8ev6nidIl_B80FnMfd&xkcb=SoDg67M37HRhBYygNx0BbzkdCdPP&fccid=ec00141efcac3522&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=e1caeb2cf4609024&bb=uFHC_idxkOdMhSd7jLvpbW8urnqsMAM7RylrkGKsYoUECzSbOjJUmcDE6PFesU1cPjBrctoXot2K9CE3nvKH-uL-y9iXtF9fr9yiBMFyIFszUPgWQgIcKA%3D%3D&xkcb=SoBU67M37HRhBYygNx0AbzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=f582b948bc9ca933&bb=uFHC_idxkOdMhSd7jLvpbflu22p7LkIctrqwWaKPBLMTiRKMJWt4u4dyG3nQu5XBDh_EdLqOfgNppNUFGipQNt8r-lzqmRDzCwUQpLe5wS3SN8KvIPXVfw%3D%3D&xkcb=SoDa67M37HRhBYygNx0HbzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=7769791f9d7d5e96&bb=uFHC_idxkOdMhSd7jLvpbQ692LcIQdspdYCymR3uZurEE1XfdnS8PcXcePoQWIP_rxtkN3BU5oJ7vhdpVWJlc_1M_1uLuqDmXOyCpmkdCPiYy3VkZthjhg%3D%3D&xkcb=SoBu67M37HRhBYygNx0GbzkdCdPP&fccid=734cb5a01ee60f80&vjs=3\n",
      "1 https://www.indeed.com/rc/clk?jk=7171f2e608fd222e&bb=uFHC_idxkOdMhSd7jLvpbRpKEqbk_OXFL3PnDLFyO1LerHdhBFzlJFwI2aUXGhVmiNVJUZba5IKAKtadN7zIJ1_8wwhHLSSvEW6YWKBJYkpiRyvFzJgmTw%3D%3D&xkcb=SoDz67M37HRhBYygNx0FbzkdCdPP&fccid=b85c5070c3d3d8c8&vjs=3\n",
      "Fetching Page number: 2\n",
      "Jobs found on page 2: 15\n",
      "2 https://www.indeed.com/rc/clk?jk=84a618e2ed4bc614&bb=61nctErOJCY0pte3mS4yPLCHNCkSEB15QrfM_6WrlL2yvotfRJAheFYt-LBGhgB3R3VA5oTWxc770ERWEqCkt4x28DXyfnZHKlBTJIovHz8LulUgGNXYDwHTSbK0tdqE&xkcb=SoAQ67M37HRx-53qa50KbzkdCdPP&fccid=1dc32badf02d6835&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=dd7a457c10fad06d&bb=61nctErOJCY0pte3mS4yPPFHjmIRdgtjq4AZa8RrjKbdKuIvM18y-SXYIKtcsTTEZqi6gXcxsX5xpRoZjUJGPySIAtX4izaI7tyNx7tgFCFOdkpltQcN_A%3D%3D&xkcb=SoCN67M37HRx-53qa50JbzkdCdPP&fccid=4f8daa48ef966ab5&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=247109f2c45cd66c&bb=61nctErOJCY0pte3mS4yPJyW3AdQSNUIH3TQ08jKiq1PPgQTAtOhZ2S-CkWNncr_8WHtl36dFfIVhFhwSRqL-l6Wb6Ttv7P6DwDOdxJUSkMqMaQKQEqvUA%3D%3D&xkcb=SoA567M37HRx-53qa50IbzkdCdPP&fccid=6c5992ae26848336&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=714dc3dd7a7299cc&bb=61nctErOJCY0pte3mS4yPNeGk4b2elGyeLLT0hKGonjBL5-IRWd8Y2IEMpv5_eselOtmgpCaTAklPISCpCOfDDFqayAzdfqne0flLEdXmo5go9q5Ogj9MLTuX4DhIkOa&xkcb=SoC367M37HRx-53qa50PbzkdCdPP&fccid=d064f91b8359ae4d&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=d1b96df5f19292fd&bb=61nctErOJCY0pte3mS4yPP8o5tf1hzJ_hNyfigGg-HG2K5KKf3NsifOcFmlOsNd1bJqFJT8M7ohUIfaVTJPG29b8xNy40g_o-G3EYV63Cxj-pezEKuXFFQ%3D%3D&xkcb=SoAD67M37HRx-53qa50ObzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=04fc256d64104902&bb=61nctErOJCY0pte3mS4yPGEgGoz5FHqqRWgH9VKT8GZGTBmZqRR_UCf6NUkny2WXbyz4-ibN0o0YnO5R0TzHby3GA7omQYprzWaBtkFwVad524Nve1PDgQ%3D%3D&xkcb=SoCe67M37HRx-53qa50NbzkdCdPP&fccid=d46039b952140fd4&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=7769791f9d7d5e96&bb=61nctErOJCY0pte3mS4yPMZrP4w8eywZNgZsnkNeUoP3R4pRfO5NAE7wiXpfsZmjSxa0KFs5pag8gTjQh2WAR0CzIZ-zdNIy2RlOWhWoyHJroqXxPmnyRQ%3D%3D&xkcb=SoAq67M37HRx-53qa50MbzkdCdPP&fccid=734cb5a01ee60f80&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=72aeef5ff8ec1f8c&bb=61nctErOJCY0pte3mS4yPEz6yBjlLlJWl0yrLRiohnV7baie2vRoWOdXkNN162u9YL2j5Qr_tWw6YzLpJqdrW0CoYKWZN9HfZDYCeVw_zGT0_SDxMVjpVw%3D%3D&xkcb=SoDD67M37HRx-53qa50DbzkdCdPP&fccid=a5b4499d9e91a5c6&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=b6706b401082dae9&bb=61nctErOJCY0pte3mS4yPHHDwD4BB49X0sn-VfL0LDKIdAonmPV5ZB6Q3plabTrSH0v1PZFC2ivy5MKMrbrZsI0i5k5xGs7Trsa-OctPBcRLWb4dhqSmew%3D%3D&xkcb=SoB367M37HRx-53qa50CbzkdCdPP&fccid=be3b11aa573faee7&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=daadb01a15d09ba4&bb=61nctErOJCY0pte3mS4yPPF1VG04CWQ80S4Oc9PwB-PNPe1p1OYhFfiUK0IbQH3W3rgLUsvLh0tOuuRuY7MFCUQ4EL8zmXwCAlUsSQPfO52sIP63jJtywarygZvwPdMp&xkcb=SoDq67M37HRx-53qa50BbzkdCdPP&fccid=13920672d471a7d1&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=6f9a2edc4d71567c&bb=61nctErOJCY0pte3mS4yPLIHwRkiAG_WACKBuJqb2BVTm5T6qISb9crA1UgB74SZUpIuvOYvMNbPuUuP07XC3Sh-l_dnkqdx3RLQ4WAatfhCL4vH5WmimmykLFcbsGtw&xkcb=SoBe67M37HRx-53qa50AbzkdCdPP&fccid=8b9872f7d5d28bd8&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=e6420cae69a882c8&bb=61nctErOJCY0pte3mS4yPNN2ucnV-kHs6V-giEBgT2tQljZCrhdFodBsXvrAmn6R0O7aIv0UsnBDsk-V-zc9Q5l_nd2fyABKIT_CQ4NIDWOcMO1_ogH-TEzzh1qFUK4K&xkcb=SoDQ67M37HRx-53qa50HbzkdCdPP&fccid=3f4a7b1ba9bdfdb0&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=22a079c54637b1cf&bb=61nctErOJCY0pte3mS4yPAqmlfdGhZpqG4GnFxQ8NflUBv9C2zfiOSlzt2eZ5IbzPElO43K-7C7JOv7PKuUGMuS64FhVH5m2zOb6GXiYcCX9WADPKwDZqA%3D%3D&xkcb=SoBk67M37HRx-53qa50GbzkdCdPP&fccid=be3b11aa573faee7&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=c742524594a2fc4f&bb=61nctErOJCY0pte3mS4yPNZ8XT4U-RuuWlE8Qe_e1IKRf5NeJZCd0hVkhcTVK2_gDf_7Glj2KH_kPnQDX7U19wIawtU7r9w_Hoa406k8pV-J1sJqHn2C6w%3D%3D&xkcb=SoD567M37HRx-53qa50FbzkdCdPP&fccid=e3775e5ec8ee0d6e&vjs=3\n",
      "2 https://www.indeed.com/rc/clk?jk=99eba59d82a9848e&bb=61nctErOJCY0pte3mS4yPCjonnOez8QkOjCW9CfOokbviXxpS6VFm5plBoxlx94YdugygocQw0wTsgOEwW0G6A5tYkgdLrZntGGVy-UCs_iOsbKtrt8AG6g57ZZlvCtn&xkcb=SoBN67M37HRx-53qa50EbzkdCdPP&fccid=7d7b563c6a3a9653&vjs=3\n",
      "Fetching Page number: 3\n",
      "Jobs found on page 3: 15\n",
      "3 https://www.indeed.com/rc/clk?jk=dc3122ee92633a65&bb=SIk001Ekv3j1TLRLwArCmgDCfGt96XrNn5hZ3BBuR8U4ywAhgHjZeeKQkE2410-Jf_F6nbDuFedmMOOlICZhl_uhQeXgUqZfzO3HysVdD0bNvry2Mh5Psg%3D%3D&xkcb=SoBV67M37HSCk3xbhJ0LbzkdCdPP&fccid=fe2d21eef233e94a&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=99eba59d82a9848e&bb=SIk001Ekv3j1TLRLwArCmp81XtYGRlnvejt14QewmX1qGznpYrOB9eAVWXP36g0-M5p8wtN5kNk2d7GwOjGf3_61O9qeW58A5WzH4IgKSwzZZ29hJaWqJPKHfQghy7hl&xkcb=SoDh67M37HSCk3xbhJ0KbzkdCdPP&fccid=7d7b563c6a3a9653&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=04fc256d64104902&bb=SIk001Ekv3j1TLRLwArCmrmQ7IFEWvN7qQny5CZa2OEGYDthhMx1JP3THelSjZ7-GGSL-bSs5CmjLLiRKlZx1F1kYZa8RSUsWbep_pw-_BoC4NVZmejKLA%3D%3D&xkcb=SoB867M37HSCk3xbhJ0JbzkdCdPP&fccid=d46039b952140fd4&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=6f9a2edc4d71567c&bb=SIk001Ekv3j1TLRLwArCmlE71mjq8DK6fSYAqHfJK1eMarqj1urxBLYG2oHhoE53_aqI2lRoLv--HCOch7pTztWG7Le6MhVxPqsGmsLxYaRIxXvwzs1dHZdEZyJrlLHn&xkcb=SoDI67M37HSCk3xbhJ0IbzkdCdPP&fccid=8b9872f7d5d28bd8&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=0d47ada5c09e40f9&bb=SIk001Ekv3j1TLRLwArCmpqrKqdYHpMkORP0AWszYqRj9rZ_F_yciE9WOAA9ML-HLkliM3czX_QZfYllOSO0iYNOrApr6E36RerE6nAp8s3SN8CMIKpv4g%3D%3D&xkcb=SoBG67M37HSCk3xbhJ0PbzkdCdPP&fccid=734cb5a01ee60f80&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=959aeb102aa9f2f4&bb=SIk001Ekv3j1TLRLwArCml1aJ1OewddXrjmf0g_HCgdovnsVKfKvnt6NOITE8f8U6Lg3XsOxv2q1QV8q7Ff7TRADpx7t6QiyAKJk2pVqDYVLa4luHQMdSafmA9iAtmgr&xkcb=SoDy67M37HSCk3xbhJ0ObzkdCdPP&fccid=ec00141efcac3522&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=a3b48a2cf17a61c4&bb=SIk001Ekv3j1TLRLwArCmgsiEIu4ZHfPD0love-Jlp0ZDRZ4ZMjgAvEP43K9-2gFRsHuaeWNPU6mp7cR45SGCQBh_hZHR4XSUWWp3D49HO9HKv3WGQMBVQ%3D%3D&xkcb=SoBv67M37HSCk3xbhJ0NbzkdCdPP&fccid=0e36880b18d20c75&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=43a4be81751e1828&bb=SIk001Ekv3j1TLRLwArCmmEoztP0Ah8R3swwY7VjK53WAITOv8LM1mrVHFGO3lf1CFfoDI4S6bSCq27tWwmReOkcGzZGcDDFx7nYahNcrNNuvwwhlgS1ocLIZsc9CZAr&xkcb=SoDb67M37HSCk3xbhJ0MbzkdCdPP&fccid=fe404d18bb9eef1e&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=fe3aa40374173daa&bb=SIk001Ekv3j1TLRLwArCmgeQezo4OX_ypxOCoKxydrzispcZxeiacUxfXbPvCicJd9GOZfVtMNdlmEJQ8xD80Qfw00geW4Ek7T0dkqwaH7GAncRVC4iDFA%3D%3D&xkcb=SoAy67M37HSCk3xbhJ0DbzkdCdPP&fccid=d46039b952140fd4&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=95512c94e04b5365&bb=SIk001Ekv3j1TLRLwArCmmGz9bSQZinu86RCiPrvvDu0VX5dKj5VrZsL6U1tssQRrE8abQuITTOuAyrTIsODiQyX2yeVKw5-fvdL4Ea6XpI4u_x9iP-pmMFpI0nwLgCK&xkcb=SoCG67M37HSCk3xbhJ0CbzkdCdPP&fccid=8a2222c2ef6a251d&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=4fcd4338dc1d27d5&bb=SIk001Ekv3j1TLRLwArCmjQaXEzblQJ3-G1LT_RsOEGastMF-Ybg-E1zWQFvNHlxKCbnnLcBqwx40At-qgODo6A1Hx9xrotD9WvhFNNGaLnPqan0pdqoFA%3D%3D&xkcb=SoAb67M37HSCk3xbhJ0BbzkdCdPP&fccid=f057e04c37cca134&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=97aaa3861083c4d9&bb=SIk001Ekv3j1TLRLwArCmgeQezo4OX_yAsqTSiWc3khwj983MkzRIvl87w4kkH-MiN190fqbF3C1ZiOjvyX849SFCcpxyBYqNnHbpdG1G-SwEFmG12rqxmWC-g-ydYmG&xkcb=SoCv67M37HSCk3xbhJ0AbzkdCdPP&fccid=82a5852c691fe3de&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=d7903cb525055ed5&bb=SIk001Ekv3j1TLRLwArCmpB0yMm6t_mDYHENS7X8KTaH6DjqLb5ybmGGsslN71EX6TNBSulak6m_pVOYu4up6LIOJhC1e7sHpuZaSs_Pzm1MceG9dS4TLw%3D%3D&xkcb=SoAh67M37HSCk3xbhJ0HbzkdCdPP&fccid=1c06920c338c6e1e&vjs=3\n",
      "3 https://www.indeed.com/rc/clk?jk=083007ccd3d81187&bb=SIk001Ekv3j1TLRLwArCmmeWrpxVyFPiK3zBplTzzukSLCVh1e48c3wFVIgnN1eWUNbmnSLm-T3fxL1AIqzkrGzp5UGMGoT322VcGHniAW7-eR12TCd20g%3D%3D&xkcb=SoCV67M37HSCk3xbhJ0GbzkdCdPP&fccid=b85c5070c3d3d8c8&vjs=3\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chrome webdriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open URL\n",
    "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
    "\n",
    "# Open a CSV file to write the job listings data\n",
    "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    theWriter = writer(f)\n",
    "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
    "    theWriter.writerow(heading)\n",
    "\n",
    "    for job_keyword in job_search_keywords:\n",
    "        for location_keyword in location_search_keywords:\n",
    "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
    "            all_jobs = []\n",
    "\n",
    "            # Find total number of pages\n",
    "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
    "            print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
    "\n",
    "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
    "                print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
    "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
    "                page_dom = get_dom(url)\n",
    "                \n",
    "                # Extract jobs from current page\n",
    "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
    "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
    "                \n",
    "                # Process each job\n",
    "                for job in jobs:\n",
    "                    job_link = base_url + get_job_link(job)\n",
    "                    job_title = get_job_title(job)\n",
    "                    company_name = get_company_name(job)\n",
    "                    company_location = get_company_location(job)\n",
    "                    salary = get_salary(job_link)\n",
    "                    job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
    "\n",
    "                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
    "\n",
    "                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
    "                    theWriter.writerow(record)  # Write the record to CSV\n",
    "\n",
    "# Closing the web browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8cc08-0300-4caf-9098-699f9b97e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# for inspection and debugging: inspect the DOM\n",
    "# or inspect element using Developer tools\n",
    "\n",
    "# initialize Chrome webdriver using ChromeDriverManager\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# open URL \n",
    "driver.get(\"https://www.indeed.com/q-USA-jobs.html?vjk=823cd7ee3c203ac3\")\n",
    "with open('indeed_jobs1_dom.txt', 'w', encoding='utf-8') as f:\n",
    "    url = paginaton_url.format(job_search_keyword[0], location_search_keyword[0], page_no)\n",
    "    time.sleep(3)\n",
    "    page_dom = get_dom(url)  # Get the DOM of the page\n",
    "\n",
    "    # Convert the DOM object to a string with pretty formatting\n",
    "    dom_html = etree.tostring(page_dom, pretty_print=True, encoding='unicode')\n",
    "\n",
    "    # Write the HTML string to the file\n",
    "    f.write(dom_html)\n",
    "\n",
    "# Save the full page DOM to a file for manual inspection\n",
    "with open('indeed_full_page.html', 'w', encoding='utf-8') as f:\n",
    "    time.sleep(4)\n",
    "    page_source = driver.page_source\n",
    "    f.write(page_source)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba926f-402c-4bd7-bf50-5aa8e0e7816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas for visualization \n",
    "# number of listings per location\n",
    "# average salary for each position \n",
    "# average salary by job AND location https://www.indeed.com/rc/clk?jk=9e205f9634a5bfdb&bb=Kaksi2QKb_5G01a0IYYWte3dB7Sd8vTC2PsDgK8Wcrl-vt-ov1bmllcNU4eq-ARapdqrzuLgp5TwUhCEMRJS0x0bX5YFJls_0xhvnMGs1P2Xvjh_c0znjkjJ1X6wishm&xkcb=SoBo67M37Ey6rjTbpx0LbzkdCdPP&fccid=2e5243142d98319d&vjs=3\n",
    "\n",
    "\n",
    "# keyword extraction\n",
    "# frequency differences for different job titles \n",
    "# tech stack keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc8ab31c-4b13-4101-8c47-44f7c715c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# import necessary modules\n",
      "import time\n",
      "import requests\n",
      "from csv import writer\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import etree as et\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "# define job and location search keywords\n",
      "job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']\n",
      "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']\n",
      "\n",
      "# define base and pagination URLs\n",
      "base_url = 'https://www.indeed.com'\n",
      "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
      "\n",
      "# check also Sweden and France \n",
      "# Just pick the 3 largest cities in each country\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "        \n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    # Example job search keywords and locations\n",
      "    job_search_keywords = ['Data Scientist']  # Replace with actual keywords\n",
      "    location_search_keywords = ['New York']  # Replace with actual locations\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_type = get_job_type(job)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    # Example job search keywords and locations\n",
      "    job_search_keywords = ['Data Scientist']  # Replace with actual keywords\n",
      "    location_search_keywords = ['New York']  # Replace with actual locations\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "def get_total_pages(url):\n",
      "    page_dom = get_dom(url)  # Get the DOM of the search results page\n",
      "    try:\n",
      "        # Adjust the XPath according to the actual HTML structure of the pagination\n",
      "        total_pages_text = page_dom.xpath('//div[@class=\"pagination\"]/a[last()]/text()')[0]\n",
      "        total_pages = int(total_pages_text.split()[-1])  # Extract the last number\n",
      "        return total_pages\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting total pages: {e}\")\n",
      "        return 1  # Return 1 if there's an error, assuming at least one page\n",
      "\n",
      "# Example usage\n",
      "search_url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York\"\n",
      "total_pages = get_total_pages(search_url)\n",
      "print(f\"Total pages found: {total_pages}\")\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_total_pages(url):\n",
      "    page_dom = get_dom(url)  # Get the DOM of the search results page\n",
      "    try:\n",
      "        # Adjust the XPath according to the actual HTML structure of the pagination\n",
      "        total_pages = page_dom.xpath('//ul[@class=\"pagination-list\"]/li[last()]/a/text()')\n",
      "        return int(total_pages[0].strip()) if total_pages else 1  # Extract total number of pages\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting total pages: {e}\")\n",
      "        return 1  # Default to 1 page if there's an error or no pagination\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = get_total_pages(first_page_url)\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "            \n",
      "            all_jobs = []\n",
      "            for page_no in range(total_pages):  # Loop through all pages\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      "\n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_total_pages(url):\n",
      "    # Load the DOM of the search results page\n",
      "    page_dom = get_dom(url)\n",
      "    try:\n",
      "        # XPath for extracting the number of pages (this is often located in the pagination section)\n",
      "        # Adjusted to fit Indeed's structure\n",
      "        total_pages = page_dom.xpath('//ul[@class=\"pagination-list\"]/li[last()-1]/a/text()')\n",
      "        \n",
      "        # Ensure that the result is converted to an integer and handle cases where no pagination is found\n",
      "        return int(total_pages[0].strip()) if total_pages else 1\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting total pages: {e}\")\n",
      "        return 1  # Default to 1 page if pagination is not available or any error occurs\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = get_total_pages(first_page_url)\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_total_pages(url):\n",
      "    # Load the DOM of the search results page\n",
      "    page_dom = get_dom(url)\n",
      "    try:\n",
      "        # XPath for extracting the number of pages (this is often located in the pagination section)\n",
      "        # Adjusted to fit Indeed's structure\n",
      "        total_pages = page_dom.xpath('//*[@id=\"jobsearch-JapanPage\"]/div/div[5]/div/div[1]/div[3]/div/div/div[2]/span[1]/text()')\n",
      "        \n",
      "        # Ensure that the result is converted to an integer and handle cases where no pagination is found\n",
      "        return int(total_pages[0].strip()) if total_pages else 1\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting total pages: {e}\")\n",
      "        return 1  # Default to 1 page if pagination is not available or any error occurs\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = get_total_pages(first_page_url)\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = driver.find_element(By.XPATH, '//*[@id=\"searchCountPages\"]').text\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "driver.quit()\n",
      "from selenium.webdriver.common.by import By  # Add this import for 'By'\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = driver.find_element(By.XPATH, '//*[@id=\"searchCountPages\"]').text\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "driver.quit()\n",
      "from selenium.webdriver.common.by import By  # Add this import for 'By'\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = driver.find_element(By.XPATH, '//*[@id=\"searchCountPages\"]').text\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            \n",
      "            # Step 1: Get the first page URL for this search\n",
      "            first_page_url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "            \n",
      "            # Step 2: Get the total number of pages\n",
      "            total_pages = 300\n",
      "            print(f\"Total pages found: {total_pages}\")\n",
      "            \n",
      "            all_jobs = []\n",
      "            for page_no in range(total_pages):  # Loop through all pages\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      "\n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the total job count to appear and extract it\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[@id=\"searchCountPages\"]'))\n",
      "    )\n",
      "    \n",
      "    # Extract text and parse the total number of jobs\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    \n",
      "    # Example of extracting the total number of jobs from the string\n",
      "    # e.g. \"Page 1 of 17 jobs\" or similar\n",
      "    total_jobs = int(job_count_text.split()[-2].replace(\",\", \"\"))  # Removes commas if any\n",
      "    print(f\"Total jobs found: {total_jobs}\")\n",
      "\n",
      "    # Each page shows 15 jobs (you can adjust this if the number per page differs)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(total_jobs / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    \n",
      "    # Extract the total number of jobs using string splitting and cleanup\n",
      "    total_jobs = int(job_count_text.split()[3].replace(\",\", \"\"))  # Adjust splitting based on the actual text format\n",
      "    print(f\"Total jobs found: {total_jobs}\")\n",
      "\n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(total_jobs / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    \n",
      "    # Extract the total number of jobs using string splitting and cleanup\n",
      "    total_jobs = int(job_count_text.split()[3].replace(\"+\", \",\", \"\"))  # Adjust splitting based on the actual text format\n",
      "    print(f\"Total jobs found: {total_jobs}\")\n",
      "\n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(total_jobs / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = job_count_text.split('+')\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = job_count_text.split('+')[0]\n",
      "    print(job_count)\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = job_count_text.split('+')[0]\n",
      "    print(int(job_count))\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = \"https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "# import necessary modules\n",
      "import time\n",
      "import requests\n",
      "from csv import writer\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import etree as et\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import math\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, , job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit()\n",
      "import math\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from csv import writer\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        driver.quit()\n",
      "\n",
      "# Main script\n",
      "job_search_keywords = [\"Data Scientist\"]  # Example job keyword\n",
      "location_search_keywords = [\"New York\"]  # Example location\n",
      "\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # Get total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "\n",
      "            for page_no in range(total_pages):  # Use total pages for the loop\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from csv import writer\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            \n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            \n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            \n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # Initialize Chrome webdriver\n",
      "    #service = Service(ChromeDriverManager().install())\n",
      "    #driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # Initialize Chrome webdriver\n",
      "    #service = Service(ChromeDriverManager().install())\n",
      "    #driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(3):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(3):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # Find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                \n",
      "                # Extract jobs from current page\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # Find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                \n",
      "                # Extract jobs from current page\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "                \n",
      "                # Process each job\n",
      "                for job in jobs:\n",
      "                    job_link = base_url + get_job_link(job)\n",
      "                    job_title = get_job_title(job)\n",
      "                    company_name = get_company_name(job)\n",
      "                    company_location = get_company_location(job)\n",
      "                    salary = get_salary(job_link)\n",
      "                    job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "\n",
      "                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                    theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# write code here to automate saving for different countries, cities, and job entries, and so on \n",
      "def scrape_indeed_jobs(driver_link, search_keyword, search_location):\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # Open CSV file to write job listings data\n",
      "    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "        theWriter = writer(f)\n",
      "        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "        theWriter.writerow(heading)\n",
      "\n",
      "        # Find total number of pages\n",
      "        total_pages = get_total_pages(search_keyword, search_location)\n",
      "        print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "        for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "            print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "            url = f\"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}\"\n",
      "            page_dom = get_dom(url)\n",
      "            \n",
      "            # Extract jobs from current page\n",
      "            jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "            print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "            \n",
      "            # Process each job\n",
      "            for job in jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "\n",
      "                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "    # Closing the web browser\n",
      "    driver.quit()\n",
      "\n",
      "# Example of calling the function\n",
      "scrape_indeed_jobs(\"https://www.indeed.com/q-USA-jobs.html\", \"Data+Scientist\", \"New+York\")\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # Find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                \n",
      "                # Extract jobs from current page\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "                \n",
      "                # Process each job\n",
      "                for job in jobs:\n",
      "                    job_link = base_url + get_job_link(job)\n",
      "                    job_title = get_job_title(job)\n",
      "                    company_name = get_company_name(job)\n",
      "                    company_location = get_company_location(job)\n",
      "                    salary = get_salary(job_link)\n",
      "                    job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "\n",
      "                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                    theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "def scrape_jobs():\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "    \n",
      "    # Open URL\n",
      "    driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "    \n",
      "    # Open a CSV file to write the job listings data\n",
      "    with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "        theWriter = writer(f)\n",
      "        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "        theWriter.writerow(heading)\n",
      "    \n",
      "        for job_keyword in job_search_keywords:\n",
      "            for location_keyword in location_search_keywords:\n",
      "                print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "                all_jobs = []\n",
      "    \n",
      "                # Find total number of pages\n",
      "                total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "                print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "    \n",
      "                for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                    print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "                    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                    page_dom = get_dom(url)\n",
      "                    \n",
      "                    # Extract jobs from current page\n",
      "                    jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                    print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "                    \n",
      "                    # Process each job\n",
      "                    for job in jobs:\n",
      "                        job_link = base_url + get_job_link(job)\n",
      "                        job_title = get_job_title(job)\n",
      "                        company_name = get_company_name(job)\n",
      "                        company_location = get_company_location(job)\n",
      "                        salary = get_salary(job_link)\n",
      "                        job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "    \n",
      "                        record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "    \n",
      "                        print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                        theWriter.writerow(record)  # Write the record to CSV\n",
      "    \n",
      "    # Closing the web browser\n",
      "    driver.quit()\n",
      "\n",
      "scrape_jobs()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # Find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                \n",
      "                # Extract jobs from current page\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "                \n",
      "                # Process each job\n",
      "                for job in jobs:\n",
      "                    job_link = base_url + get_job_link(job)\n",
      "                    job_title = get_job_title(job)\n",
      "                    company_name = get_company_name(job)\n",
      "                    company_location = get_company_location(job)\n",
      "                    salary = get_salary(job_link)\n",
      "                    job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "\n",
      "                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                    theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# write code here to automate saving for different countries, cities, and job entries, and so on \n",
      "\n",
      "# Modify get_total_pages and get_dom functions to accept driver as a parameter\n",
      "def get_total_pages(job_keyword, location_keyword, driver):\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        job_count_text = job_count_element.text\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "        jobs_per_page = 15\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "def get_dom(url, driver):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "def scrape_indeed_jobs(search_keyword, search_location):\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # Open CSV file to write job listings data\n",
      "    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "        theWriter = writer(f)\n",
      "        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "        theWriter.writerow(heading)\n",
      "\n",
      "        # Find total number of pages\n",
      "        total_pages = get_total_pages(search_keyword, search_location, driver)\n",
      "        print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "        for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "            print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "            url = f\"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}\"\n",
      "            page_dom = get_dom(url, driver)\n",
      "            \n",
      "            # Extract jobs from current page\n",
      "            jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "            print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "            \n",
      "            # Process each job\n",
      "            for job in jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description for this job\n",
      "\n",
      "                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "    # Closing the web browser\n",
      "    driver.quit()\n",
      "\n",
      "# Example of calling the function\n",
      "scrape_indeed_jobs(\"Data+Scientist\", \"New+York\")\n",
      "def scrape_indeed_jobs(search_keyword, search_location):\n",
      "    # Initialize Chrome webdriver\n",
      "    service = Service(ChromeDriverManager().install())\n",
      "    driver = webdriver.Chrome(service=service)\n",
      "\n",
      "    # Open CSV file to write job listings data\n",
      "    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "        theWriter = writer(f)\n",
      "        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "        theWriter.writerow(heading)\n",
      "\n",
      "        # Find total number of pages\n",
      "        total_pages = get_total_pages(search_keyword, search_location, driver)\n",
      "        print(f\"Total pages found: {total_pages}\")  # Print total pages\n",
      "\n",
      "        for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "            print(f\"Fetching Page number: {page_no + 1}\")  # Display page number (1-based)\n",
      "            url = f\"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}\"\n",
      "            page_dom = get_dom(url, driver)\n",
      "            \n",
      "            # Extract jobs from current page\n",
      "            jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "            print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\")  # Print number of jobs found\n",
      "            \n",
      "            # Process each job\n",
      "            for job in jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link, driver)  # Pass the driver\n",
      "                job_desc = get_job_desc(job_link, driver)  # Pass the driver\n",
      "\n",
      "                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no + 1, job_link)  # Print page number (1-based index) and job link\n",
      "                theWriter.writerow(record)  # Write the record to CSV\n",
      "\n",
      "    # Closing the web browser\n",
      "    driver.quit()\n",
      "\n",
      "def get_salary(job_link, driver):  # Accept driver as an argument\n",
      "    job_dom = get_dom(job_link, driver)  # Pass the driver\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return ''.join(salary).strip() if salary else \"Not specified\"\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting salary for {job_link}: {e}\")\n",
      "        return \"Error\"\n",
      "\n",
      "def get_job_desc(job_link, driver):  # Accept driver as an argument\n",
      "    job_dom = get_dom(job_link, driver)  # Pass the driver\n",
      "    # Extract job description logic goes here\n",
      "    # Example:\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//div[@class=\"jobsearch-jobDescriptionText\"]//text()')\n",
      "        return ''.join(job_desc).strip() if job_desc else \"No description available\"\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting job description for {job_link}: {e}\")\n",
      "        return \"Error\"\n",
      "\n",
      "# Example of calling the function\n",
      "scrape_indeed_jobs(\"Data+Scientist\", \"New+York\")\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "                # are these all jobs or just all the jobs for one page? \n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit().\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "                # are these all jobs or just all the jobs for one page? \n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# import necessary modules\n",
      "import time\n",
      "import math\n",
      "import requests\n",
      "from csv import writer\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import etree as et\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from selenium.webdriver.common.by import By\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "# define job and location search keywords\n",
      "job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst'] # check also Sweden and France \n",
      "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago'] # Just pick the 3 largest cities in each country \n",
      "\n",
      "# define base and pagination URLs\n",
      "base_url = 'https://www.indeed.com'\n",
      "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job title\n",
      "def get_job_title(job):\n",
      "    try:\n",
      "        job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "    except Exception:\n",
      "        job_title = 'Not available'\n",
      "    return job_title\n",
      "\n",
      "# Function to extract the company name\n",
      "def get_company_name(job):\n",
      "    try:\n",
      "        company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_name = 'Not available'\n",
      "    return company_name\n",
      "\n",
      "# Function to extract the company location\n",
      "def get_company_location(job):\n",
      "    try:\n",
      "        company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "    except Exception:\n",
      "        company_location = 'Not available'\n",
      "    return company_location\n",
      "\n",
      "# Function to extract salary\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to get total number of pages\n",
      "def get_total_pages(job_keyword, location_keyword):\n",
      "\n",
      "    # URL of Indeed job search\n",
      "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "    driver.get(url)\n",
      "\n",
      "    try:\n",
      "        # Wait for the element containing the job count to appear\n",
      "        job_count_element = WebDriverWait(driver, 10).until(\n",
      "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "        )\n",
      "        \n",
      "        # Extract the text from the element\n",
      "        job_count_text = job_count_element.text\n",
      "        print(f\"Job count text: {job_count_text}\")\n",
      "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
      "\n",
      "        # Each page shows 15 jobs\n",
      "        jobs_per_page = 15\n",
      "\n",
      "        # Calculate the total number of pages\n",
      "        total_pages = math.ceil(job_count / jobs_per_page)\n",
      "        print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "        return total_pages\n",
      "\n",
      "    except Exception as e:\n",
      "        print(f\"Error extracting job count: {e}\")\n",
      "        return 0  # Return 0 if there's an error\n",
      "\n",
      "    finally:\n",
      "        # Close the browser\n",
      "        #driver.quit()\n",
      "        pass\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "                # are these all jobs or just all the jobs for one page? \n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "\n",
      "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
      "            # do we really want this to start the driver and stop it\n",
      "\n",
      "            for page_no in range(total_pages):  # Modify range to get more pages if needed\n",
      "                print(f\"Page number: {page_no}\")\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "                # are these all jobs or just all the jobs for one page? \n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(page_no, job_link)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# import necessary modules\n",
      "import time\n",
      "import requests\n",
      "from csv import writer\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import etree as et\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "# define job and location search keywords\n",
      "job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']\n",
      "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']\n",
      "\n",
      "# define base and pagination URLs\n",
      "base_url = 'https://www.indeed.com'\n",
      "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
      "\n",
      "# check also Sweden and France \n",
      "# Just pick the 3 largest cities in each country\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_skills(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use a refined XPath to get all relevant text within the profile insights\n",
      "        skills = job_dom.xpath('//div[contains(@class, \"profile-insights\")]//text()')\n",
      "        return \" \".join(skills).strip() if skills else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# functions to extract job title\n",
      "def get_job_title(job):\n",
      "   try:\n",
      "       job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_title = 'Not available'\n",
      "   return job_title\n",
      "\n",
      "# functions to extract the company name\n",
      "def get_company_name(job):\n",
      "   try:\n",
      "       company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "       #company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_name = 'Not available'\n",
      "   return company_name\n",
      "\n",
      "# functions to extract the company location\n",
      "def get_company_location(job):\n",
      "   try:\n",
      "       company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "       #company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_location = 'Not available'\n",
      "   return company_location\n",
      "\n",
      "# functions to extract salary information\n",
      "'''\n",
      "def get_salary(job):\n",
      "   try:\n",
      "       salary = job.xpath('./descendant::span[@class=\"estimated-salary\"]/span/text()')\n",
      "   except Exception as e:\n",
      "       salary = 'Not available'\n",
      "   if len(salary) == 0:\n",
      "       try:\n",
      "           salary = job.xpath('./descendant::div[@class=\"metadata salary-snippet-container\"]/div/text()')[0]\n",
      "       except Exception as e:\n",
      "           salary = 'Not available'\n",
      "   else:\n",
      "       salary = salary[0]\n",
      "   return salary\n",
      "'''\n",
      "\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "        \n",
      "# functions to extract job type\n",
      "def get_job_type(job):\n",
      "   try:\n",
      "       job_type = job.xpath('./descendant::div[@class=\"metadata\"]/div/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_type = 'Not available'\n",
      "   return job_type\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_type', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    # Example job search keywords and locations\n",
      "    job_search_keywords = ['Data Scientist']  # Replace with actual keywords\n",
      "    location_search_keywords = ['New York']  # Replace with actual locations\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_type = get_job_type(job)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      "                skills = get_skills(job_link)  # Extract skills\n",
      "\n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_type, job_desc]\n",
      "\n",
      "                print(salary)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# import necessary modules\n",
      "import time\n",
      "import requests\n",
      "from csv import writer\n",
      "from bs4 import BeautifulSoup\n",
      "from lxml import etree as et\n",
      "from selenium import webdriver\n",
      "from selenium.webdriver.chrome.service import Service\n",
      "from webdriver_manager.chrome import ChromeDriverManager\n",
      "# define job and location search keywords\n",
      "job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']\n",
      "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']\n",
      "\n",
      "# define base and pagination URLs\n",
      "base_url = 'https://www.indeed.com'\n",
      "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
      "\n",
      "# check also Sweden and France \n",
      "# Just pick the 3 largest cities in each country\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_skills(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use a refined XPath to get all relevant text within the profile insights\n",
      "        skills = job_dom.xpath('//div[contains(@class, \"profile-insights\")]//text()')\n",
      "        return \" \".join(skills).strip() if skills else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# functions to extract job title\n",
      "def get_job_title(job):\n",
      "   try:\n",
      "       job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_title = 'Not available'\n",
      "   return job_title\n",
      "\n",
      "# functions to extract the company name\n",
      "def get_company_name(job):\n",
      "   try:\n",
      "       company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "       #company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_name = 'Not available'\n",
      "   return company_name\n",
      "\n",
      "# functions to extract the company location\n",
      "def get_company_location(job):\n",
      "   try:\n",
      "       company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "       #company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_location = 'Not available'\n",
      "   return company_location\n",
      "\n",
      "# functions to extract salary information\n",
      "'''\n",
      "def get_salary(job):\n",
      "   try:\n",
      "       salary = job.xpath('./descendant::span[@class=\"estimated-salary\"]/span/text()')\n",
      "   except Exception as e:\n",
      "       salary = 'Not available'\n",
      "   if len(salary) == 0:\n",
      "       try:\n",
      "           salary = job.xpath('./descendant::div[@class=\"metadata salary-snippet-container\"]/div/text()')[0]\n",
      "       except Exception as e:\n",
      "           salary = 'Not available'\n",
      "   else:\n",
      "       salary = salary[0]\n",
      "   return salary\n",
      "'''\n",
      "\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "        \n",
      "# functions to extract job type\n",
      "def get_job_type(job):\n",
      "   try:\n",
      "       job_type = job.xpath('./descendant::div[@class=\"metadata\"]/div/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_type = 'Not available'\n",
      "   return job_type\n",
      "# Function to get DOM from given URL\n",
      "def get_dom(url):\n",
      "    driver.get(url)\n",
      "    time.sleep(3)  # Ensure page loads\n",
      "    page_content = driver.page_source\n",
      "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
      "    dom = et.HTML(str(product_soup))\n",
      "    return dom\n",
      "\n",
      "# Function to extract job link\n",
      "def get_job_link(job):\n",
      "    try:\n",
      "        return job.xpath('./descendant::h2/a/@href')[0]\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# Function to extract job description\n",
      "def get_job_desc(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
      "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "def get_skills(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use a refined XPath to get all relevant text within the profile insights\n",
      "        skills = job_dom.xpath('//div[contains(@class, \"profile-insights\")]//text()')\n",
      "        return \" \".join(skills).strip() if skills else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "\n",
      "# functions to extract job title\n",
      "def get_job_title(job):\n",
      "   try:\n",
      "       job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_title = 'Not available'\n",
      "   return job_title\n",
      "\n",
      "# functions to extract the company name\n",
      "def get_company_name(job):\n",
      "   try:\n",
      "       company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
      "       #company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_name = 'Not available'\n",
      "   return company_name\n",
      "\n",
      "# functions to extract the company location\n",
      "def get_company_location(job):\n",
      "   try:\n",
      "       company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
      "       #company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
      "   except Exception as e:\n",
      "       company_location = 'Not available'\n",
      "   return company_location\n",
      "\n",
      "# functions to extract salary information\n",
      "'''\n",
      "def get_salary(job):\n",
      "   try:\n",
      "       salary = job.xpath('./descendant::span[@class=\"estimated-salary\"]/span/text()')\n",
      "   except Exception as e:\n",
      "       salary = 'Not available'\n",
      "   if len(salary) == 0:\n",
      "       try:\n",
      "           salary = job.xpath('./descendant::div[@class=\"metadata salary-snippet-container\"]/div/text()')[0]\n",
      "       except Exception as e:\n",
      "           salary = 'Not available'\n",
      "   else:\n",
      "       salary = salary[0]\n",
      "   return salary\n",
      "'''\n",
      "\n",
      "def get_salary(job_link):\n",
      "    job_dom = get_dom(job_link)\n",
      "    try:\n",
      "        # Use the provided XPath to get the salary text\n",
      "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
      "        return \" \".join(salary).strip() if salary else 'Not available'\n",
      "    except Exception:\n",
      "        return 'Not available'\n",
      "        \n",
      "# functions to extract job type\n",
      "def get_job_type(job):\n",
      "   try:\n",
      "       job_type = job.xpath('./descendant::div[@class=\"metadata\"]/div/text()')[0]\n",
      "   except Exception as e:\n",
      "       job_type = 'Not available'\n",
      "   return job_type\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit() Here is the other code where I would like to incorporate it: # Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            \n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "# Initialize Chrome webdriver\n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# URL of Indeed job search\n",
      "url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
      "driver.get(url)\n",
      "\n",
      "try:\n",
      "    # Wait for the element containing the job count to appear (using a more flexible XPath)\n",
      "    job_count_element = WebDriverWait(driver, 10).until(\n",
      "        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
      "    )\n",
      "    \n",
      "    # Extract the text from the element\n",
      "    job_count_text = job_count_element.text\n",
      "    print(f\"Job count text: {job_count_text}\")\n",
      "    job_count = int(job_count_text.split('+')[0])\n",
      "    \n",
      "    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)\n",
      "    jobs_per_page = 15\n",
      "\n",
      "    # Calculate the total number of pages\n",
      "    total_pages = math.ceil(job_count / jobs_per_page)\n",
      "    print(f\"Total pages: {total_pages}\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"Error extracting job count: {e}\")\n",
      "\n",
      "# Close the browser\n",
      "driver.quit() \n",
      "service = Service(ChromeDriverManager().install())\n",
      "driver = webdriver.Chrome(service=service)\n",
      "\n",
      "# Open URL\n",
      "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
      "\n",
      "# Open a CSV file to write the job listings data\n",
      "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
      "    theWriter = writer(f)\n",
      "    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']\n",
      "    theWriter.writerow(heading)\n",
      "\n",
      "    for job_keyword in job_search_keywords:\n",
      "        for location_keyword in location_search_keywords:\n",
      "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
      "            all_jobs = []\n",
      "\n",
      "            # find total number of pages\n",
      "            \n",
      "            for page_no in range(2):  # Modify range to get more pages if needed\n",
      "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
      "                page_dom = get_dom(url)\n",
      "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
      "                all_jobs.extend(jobs)\n",
      "\n",
      "            seen_links = set()\n",
      "            for job in all_jobs:\n",
      "                job_link = base_url + get_job_link(job)\n",
      "                if job_link in seen_links:\n",
      "                    continue  # Skip duplicates\n",
      "                seen_links.add(job_link)\n",
      "\n",
      "                job_title = get_job_title(job)\n",
      "                company_name = get_company_name(job)\n",
      "                company_location = get_company_location(job)\n",
      "                salary = get_salary(job_link)\n",
      "                job_desc = get_job_desc(job_link)  # Extract job description from the job link\n",
      " \n",
      "                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
      "\n",
      "                print(record)\n",
      "                \n",
      "                theWriter.writerow(record)\n",
      "\n",
      "# Closing the web browser\n",
      "driver.quit()\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37569248-dcb3-4c70-8c89-009c1e675ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
