{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5df94f-3cd2-4290-85cd-ef3c40551d08",
   "metadata": {},
   "source": [
    "# Indeed job listing data - Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f858da2c-1e97-4070-b1fd-994b71cb62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages \n",
    "#!pip install missingno\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c9ebe-f86b-44b9-9ba5-81b68f34c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3903d0-00c6-4b49-b196-aa8f6958d564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmafrid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Constants and configurations\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded (run this once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "DATA_PATH = 'output/indeed_jobs_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a117e1-5873-4888-85fd-4298338e61aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_SWE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     18\u001b[0m currency_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSWE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEK\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Swedish Krona\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEUR\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Euro\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEUR\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Euro\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSD\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# US Dollar\u001b[39;00m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Dictionary of data frames \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSWE\u001b[39m\u001b[38;5;124m'\u001b[39m: df_SWE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRA\u001b[39m\u001b[38;5;124m'\u001b[39m: df_FRA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITA\u001b[39m\u001b[38;5;124m'\u001b[39m: df_ITA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m: df_USA}\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Dictionary with common software/programming tools keywords \u001b[39;00m\n\u001b[1;32m     29\u001b[0m software_keywords \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgramming Languages\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m r \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjavascript\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc++\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc#\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruby\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswift\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkotlin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscala\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatlab\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m go \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypescript\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     ]\n\u001b[1;32m     66\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_SWE' is not defined"
     ]
    }
   ],
   "source": [
    "# Keyword dictionaries and other configurations \n",
    "\n",
    "# Dictionary specifying column names and desired data types\n",
    "dtype_dict = {\n",
    "    'page': 'int64',  \n",
    "    'country': 'string', \n",
    "    'job_link': 'object', \n",
    "    'search_keyword': 'string', \n",
    "    'search_location': 'string', \n",
    "    'job_title': 'string', \n",
    "    'company_name': 'string', \n",
    "    'company_location': 'object', \n",
    "    'salary': 'object', \n",
    "    'job_description': 'string'\n",
    "}\n",
    "\n",
    "# Dictionary of data frames and their corresponding currencies\n",
    "currency_mapping = {\n",
    "    'SWE': 'SEK',  # Swedish Krona\n",
    "    'FRA': 'EUR',  # Euro\n",
    "    'ITA': 'EUR',  # Euro\n",
    "    'USA': 'USD'   # US Dollar\n",
    "}\n",
    "\n",
    "# Dictionary with common software/programming tools keywords \n",
    "software_keywords = {\n",
    "    'Programming Languages': [\n",
    "        'python', ' r ', 'sql', 'javascript', 'java', 'c++', 'c#', 'ruby', 'swift', 'kotlin', 'scala', 'matlab', 'sas', 'stata', ' go ', 'php', 'typescript', 'rust', 'bash'\n",
    "    ],\n",
    "    'Data Analysis and Manipulation': [\n",
    "        'excel', 'pandas', 'numpy', 'dplyr', 'tidyverse', 'julia', 'matlab', 'stata'\n",
    "    ],\n",
    "    'Machine Learning and Statistical Modeling': [\n",
    "        'scikit-learn', 'tensorflow', 'keras', 'pytorch', 'xgboost', 'catboost', 'lightgbm', 'mlpack', 'caret', 'mlr', 'weka', 'statsmodels'\n",
    "    ],\n",
    "    'Data Visualization and Business Intelligence (BI) Tools': [\n",
    "        'tableau', 'power bi', 'matplotlib', 'seaborn', 'd3.js', 'looker', 'plotly', 'ggplot2', 'qlik', 'sap', 'looker studio', 'superset', 'metabase'\n",
    "    ],\n",
    "    'Big Data Technologies': [\n",
    "        'spark', 'hadoop', 'bigquery', 'redshift', 'snowflake', 'databricks', 'hive', 'kafka', 'hdfs', 'flink', 'storm'\n",
    "    ],\n",
    "    'Database Management Systems (DBMS)': [\n",
    "        'mysql', 'postgresql', 'mongodb', 'cassandra', 'oracle', 'microsoft sql server', 'firebase', 'db2', 'couchbase', 'neo4j', 'redis', 'couchdb', 'mariadb'\n",
    "    ],\n",
    "    'Cloud Computing': [\n",
    "        'aws', 'azure', 'google cloud', 'gcp', 'ibm cloud', 'oracle cloud', 'digitalocean', 'heroku'\n",
    "    ],\n",
    "    'Development Tools': [\n",
    "        'git', 'docker', 'vscode', 'jupyter', 'pycharm', 'rstudio', 'eclipse', 'netbeans', 'intellij idea', 'notepad++', 'sublime text', 'atom'\n",
    "    ],\n",
    "    'Version Control and Collaboration': [\n",
    "        'github', 'gitlab', 'bitbucket', 'jira', 'confluence', 'slack', 'trello', 'microsoft teams', 'asana', 'notion'\n",
    "    ],\n",
    "    'Containerization and Orchestration': [\n",
    "        'docker', 'kubernetes', 'openshift', 'mesos', 'rancher', 'nomad'\n",
    "    ],\n",
    "    'Workflow Management': [\n",
    "        'airflow', 'luigi', 'prefect', 'kubeflow'\n",
    "    ],\n",
    "    'Data Science Platforms': [\n",
    "        'databricks', 'knime', 'h2o.ai', 'rapidminer', 'datarobot', 'mlflow'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e682b2-db21-4c5f-9a6a-6d1fa56e0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "\n",
    "# Salary conversion function to handle both thousand separators and decimal points\n",
    "def convert_salary(value):\n",
    "    # Converts salary strings with thousand separators or decimal points into a float.\n",
    "    return float(value.replace('\\xa0', '').replace(' ', '').replace(',', '').replace('.', '').replace('..', '.'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and make lowercase\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "def tokenize_and_filter(text, stop_words):\n",
    "    # Tokenization: split text into words and remove stopwords\n",
    "    tokens = text.split()\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2875ca1-3861-49e7-ab8f-514db37fffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level functions \n",
    "\n",
    "def merge_US_cities(cities):\n",
    "    \"\"\"\n",
    "    Merges job listings from multiple US cities into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - cities: List of city names (strings) to merge.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing job listings from all specified US cities.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data for the first city and add the 'country' column manually\n",
    "    df_NY = pd.read_csv(f\"{DATA_PATH}{'USA_'}{cities[0]}.csv\")\n",
    "    df_NY['country'] = 'USA'  # Add the 'country' column to match format\n",
    "    print(\"Loaded data for\", cities[0])\n",
    "\n",
    "    # Load data for other cities\n",
    "    df_LA = pd.read_csv(f\"{DATA_PATH}{'USA_'}{cities[1]}.csv\")\n",
    "    df_CHI = pd.read_csv(f\"{DATA_PATH}{'USA_'}{cities[2]}.csv\")\n",
    "\n",
    "    # Ensure consistent column order across DataFrames\n",
    "    desired_order = df_LA.columns.tolist()\n",
    "    df_NY = df_NY[desired_order]\n",
    "    print(\"Column order for consistency:\", desired_order)\n",
    "\n",
    "    # Concatenate the DataFrames\n",
    "    df_USA = pd.concat([df_NY, df_LA, df_CHI], ignore_index=True)\n",
    "\n",
    "    # Verify column order consistency \n",
    "    assert df_USA.columns.tolist() == desired_order, \"Column order mismatch!\"\n",
    "\n",
    "    return df_USA\n",
    "\n",
    "# unique() prints the unique values, nunique() prints the number of unique values\n",
    "def check_duplicates(data):\n",
    "    # The number of rows should be equal to the number of unique job links, etc \n",
    "    # Get the number of rows \n",
    "    num_rows = data.shape[0]\n",
    "    # Print the number of rows\n",
    "    print(f'The DataFrame has {num_rows} rows.')\n",
    "    print(data.nunique()) \n",
    "    # Check for duplicates in all columns\n",
    "    duplicates = data.duplicated(keep=False)\n",
    "    # Print duplicate rows \n",
    "    print(data[duplicates])\n",
    "\n",
    "def desc_categorical(data):\n",
    "    # Get frequency counts for each categorical column\n",
    "    string_columns = data.select_dtypes(include='string').drop(columns='job_description') # Skip job description! \n",
    "    # Get frequency counts for the categorical columns with mixed data types (strings and numbers)\n",
    "    object_columns = data.select_dtypes(include='object').drop(columns='job_link')\n",
    "\n",
    "    # Loop through the columns and print value counts\n",
    "    for col in string_columns.columns:\n",
    "        print(f'Value counts for column: {col}\\n{string_columns[col].value_counts()}\\n')\n",
    "    for col in object_columns.columns:\n",
    "        print(f'Value counts for column: {col}\\n{object_columns[col].value_counts()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac5d48-d0e4-4748-99e4-aab452600a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for extracting salary information \n",
    "\n",
    "# Format, clean, and fix columns for salary column \n",
    "def clean_columns(data):\n",
    "    # Remove + signs and replace them with spaces in 'search_keyword' and 'search_location'\n",
    "    data[['search_keyword', 'search_location']] = data[['search_keyword', 'search_location']].replace({r'\\+': ' '}, regex=True)\n",
    "    \n",
    "    # Remove all newline characters from 'job_description'\n",
    "    data['job_description'] = data['job_description'].replace({r'\\n': ' '}, regex=True)\n",
    "    \n",
    "    # Extract salary numbers using regex\n",
    "    # This regex captures numbers with commas, spaces, and periods, handling both American and European formats\n",
    "    data['salary'] = data['salary'].astype(str)\n",
    "    data['salary_num'] = data['salary'].apply(lambda x: re.findall(r'\\d{1,3}(?:[,\\s]\\d{3})*(?:\\.\\d+)?', x))\n",
    "    \n",
    "    # Replace empty lists with NaN in 'salary_num'\n",
    "    data['salary_num'] = data['salary_num'].apply(lambda x: x if x else np.nan)\n",
    "    \n",
    "    # Create 'salary_num_low' and 'salary_num_high' by extracting and cleaning the numbers\n",
    "    # If there is only one number put it in both low and high column\n",
    "    data['salary_num_low'] = data['salary_num'].apply(lambda x: convert_salary(x[0]) if isinstance(x, list) and len(x) > 0 else np.nan)\n",
    "    data['salary_num_high'] = data['salary_num'].apply(lambda x: convert_salary(x[0]) if isinstance(x, list) and len(x) == 1 else convert_salary(x[1]) if isinstance(x, list) and len(x) > 1 else np.nan)\n",
    "\n",
    "    # Extract time period from 'salary' column using regex\n",
    "    # par an since 'an' is an English word \n",
    "    data['time_period'] = data['salary'].str.extract(r'(hour|year|month|week|day|ora|anno|mese|settimana|giorno|heure|par an|mois|semaine|jour|månad)')\n",
    "\n",
    "    return data\n",
    "\n",
    "def convert_salary_to_monthly(row, salary_column):\n",
    "    # Dictionary to map time periods (in different languages) to their monthly conversion factor\n",
    "    time_period_map = {\n",
    "        'hour': 160, 'ora': 160, 'heure': 160,\n",
    "        'year': 1/12, 'anno': 1/12, 'par an': 1/12,\n",
    "        'week': 4, 'settimana': 4, 'semaine': 4,\n",
    "        'day': 20, 'giorno': 20, 'jour': 20,\n",
    "        'month': 1, 'mese': 1, 'mois': 1, 'månad': 1\n",
    "    }\n",
    "    \n",
    "    time_period = row['time_period']\n",
    "    \n",
    "    # Check if 'time_period' is a valid string and map it to conversion factor, otherwise return NaN\n",
    "    if isinstance(time_period, str):\n",
    "        time_period = time_period.lower()\n",
    "        return row[salary_column] * time_period_map.get(time_period, np.nan)\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "# Function to apply salary conversion for min and max salary\n",
    "def apply_salary_conversion(df, currency):\n",
    "    df['min_salary_month'] = df.apply(lambda row: convert_salary_to_monthly(row, 'salary_num_low'), axis=1)\n",
    "    df['max_salary_month'] = df.apply(lambda row: convert_salary_to_monthly(row, 'salary_num_high'), axis=1)\n",
    "    df['currency'] = currency  # Add currency column\n",
    "    return df\n",
    "\n",
    "# Function to clean DataFrames, add a currency column, and calculate salary per month\n",
    "def clean_and_add_currency_and_salaries(df, currency):\n",
    "    cleaned_df = clean_columns(df)  # Clean the DataFrame\n",
    "    cleaned_df['currency'] = currency  # Add currency column\n",
    "    # Calculate min and max salary per month\n",
    "    cleaned_df['min_salary_month'] = cleaned_df.apply(lambda row: convert_salary_to_monthly(row, 'salary_num_low'), axis=1)\n",
    "    cleaned_df['max_salary_month'] = cleaned_df.apply(lambda row: convert_salary_to_monthly(row, 'salary_num_high'), axis=1)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70503661-30db-4fa2-be49-eddbccdb4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for extracting info from job descriptions \n",
    "def extract_keywords(df, country, language):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df: DataFrame containing job descriptions and search keywords.\n",
    "    - country: String representing the country to filter by.\n",
    "    - language: language to filter by (to ensure correct stopwords are removed). \n",
    "\n",
    "    Returns:\n",
    "    - A list with most common keywords? \n",
    "    \"\"\"\n",
    "    \n",
    "    # Always include English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Add additional stopwords based on the specified language\n",
    "    if language == 'french':\n",
    "        stop_words.update(stopwords.words('french'))\n",
    "    elif language == 'italian':\n",
    "        stop_words.update(stopwords.words('italian'))\n",
    "    elif language == 'swedish':\n",
    "        stop_words.update(stopwords.words('swedish'))\n",
    "    elif language == 'english':\n",
    "        # English stopwords are already included at the top\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language.\")\n",
    "\n",
    "    # Filter the DataFrame for the specified country\n",
    "    df_country = df[df['country'] == country].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    # Use .loc to assign new columns\n",
    "    df_country.loc[:, 'cleaned_description'] = df_country['job_description'].apply(preprocess_text)\n",
    "    #df_country.loc[:, 'tokens'] = df_country['cleaned_description'].apply(tokenize_and_filter)\n",
    "    df_country.loc[:, 'tokens'] = df_country['cleaned_description'].apply(lambda text: tokenize_and_filter(text, stop_words))\n",
    "\n",
    "    # Flatten the list of tokens and count frequencies\n",
    "    all_tokens = [token for sublist in df_country['tokens'] for token in sublist]\n",
    "    word_counts = Counter(all_tokens)\n",
    "\n",
    "    # Get the top 10 keywords\n",
    "    common_keywords = word_counts.most_common(10)  \n",
    "    return (common_keywords, all_tokens)\n",
    "\n",
    "def plot_common_keywords(common_keywords, country):\n",
    "    \"\"\"\n",
    "    Plots the most common keywords from job descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - common_keywords: List of tuples (keyword, frequency).\n",
    "    - country: Name of the country for labeling the plot.\n",
    "    \"\"\"\n",
    "    # Unzip the list of tuples into two lists: words and counts\n",
    "    words, counts = zip(*common_keywords)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "    plt.bar(words, counts, color='skyblue')  # Bar plot\n",
    "    plt.xlabel('Keywords', fontsize=14)  # Label for x-axis\n",
    "    plt.ylabel('Frequency', fontsize=14)  # Label for y-axis\n",
    "    plt.title(f'Most Common Keywords in Job Descriptions - {country}', fontsize=16)  # Title of the plot\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjust layout to make room for rotated labels\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "def count_keywords(df, country):\n",
    "    \"\"\"\n",
    "    Counts the occurrences of keywords in job descriptions by category and sub-category for a specific country,\n",
    "    creating separate entries for each keyword and its associated search keyword.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing job descriptions and search keywords.\n",
    "    - country: String representing the country to filter by.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with categories, sub-categories, keyword counts, associated search keywords, and country.\n",
    "    \"\"\"\n",
    "    # Prepare the DataFrame list to store individual entries\n",
    "    data = []\n",
    "\n",
    "    # Filter DataFrame by country\n",
    "    filtered_df = df[df['country'] == country]\n",
    "    \n",
    "    # Flatten the keywords into a single list with their categories\n",
    "    category_keywords = [(category, keyword) for category, keywords in software_keywords.items() for keyword in keywords]\n",
    "\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        job_description = row['job_description'].lower()  # Access job description\n",
    "        search_keyword = row['search_keyword']  # Access associated search keyword\n",
    "        \n",
    "        for category, keyword in category_keywords:\n",
    "            count = job_description.count(keyword)\n",
    "            if count > 0:  # Only record non-zero counts\n",
    "                data.append({\n",
    "                    'Category': category,\n",
    "                    'Keyword': keyword,\n",
    "                    'Count': count,\n",
    "                    'Search Keyword': search_keyword,\n",
    "                    'Country': country  \n",
    "                })\n",
    "\n",
    "    # Create a df from the collected data\n",
    "    result_df = pd.DataFrame(data)\n",
    "\n",
    "    # Group by relevant columns and sum the counts\n",
    "    result_df = result_df.groupby(['Category', 'Keyword', 'Search Keyword', 'Country'], as_index=False).sum()\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903b625-1068-44b8-9a74-dcae6467480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract info about interview process\n",
    "\n",
    "def extract_interview_info(df):\n",
    "    \"\"\"\n",
    "    Extracts interview process information from job descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing job listings.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with extracted interview information.\n",
    "    \"\"\"\n",
    "    # Initialize a list to hold the extracted information\n",
    "    interview_info = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        job_desc = row['job_description']\n",
    "        \n",
    "        # Extract information based on keywords and patterns\n",
    "        interview_details = {\n",
    "            'job_id': row['job_id'],\n",
    "            'phone_screening': bool(re.search(r'phone screening|phone interview|screening call', job_desc, re.IGNORECASE)),\n",
    "            'coding_assessment': bool(re.search(r'coding test|coding interview|programming assessment|technical assessment|live coding challenge', job_desc, re.IGNORECASE)),\n",
    "            'case_study': bool(re.search(r'case study|take-home assignment', job_desc, re.IGNORECASE)),\n",
    "            'on_site_interview': bool(re.search(r'on-site interview|final round|in-person interview', job_desc, re.IGNORECASE)),\n",
    "            'presentation': bool(re.search(r'presentation|project presentation|technical presentation', job_desc, re.IGNORECASE)),\n",
    "        }\n",
    "        \n",
    "        interview_info.append(interview_details)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    interview_info_df = pd.DataFrame(interview_info)\n",
    "\n",
    "    return interview_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae22556-fab9-47c0-9e67-b5a8679d50ee",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Import the data scraped using scraper-countries.py (Sept 20-21 2024), for the following countries: USA, Sweden, France, and Italy. These datasets contain the job listings for the 3 largest cities in respective country, for the job titles _Data Scientist_, _Data Analyst_, _Product Analyst_, and _BI Analyst_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994442d7-827d-4baa-a38f-1d3bd9c83b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv files with scraped data for resp. country \n",
    "# Sweden \n",
    "df_SWE = pd.read_csv(f\"{DATA_PATH}{'Sweden'}.csv\")\n",
    "# France \n",
    "df_FRA = pd.read_csv(f\"{DATA_PATH}{'France'}.csv\")\n",
    "# Italy\n",
    "df_ITA =pd.read_csv(f\"{DATA_PATH}{'Italy'}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77380f-2c63-4606-9d5e-be27da335d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA \n",
    "df_USA = merge_US_cities(['NY', 'LA', 'CHI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b1427-0bc7-46c9-9aa3-0beee7f08aea",
   "metadata": {},
   "source": [
    "## Initial inspection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61551cf-c838-4fb0-a1fa-8959bfcf0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SWE.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b9eaf-f713-4268-9942-7a4538ff9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FRA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877d22d-50bc-40d6-a67b-7909e7b6d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ITA.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ec993-0ccd-4f0a-b2b3-c71ca005d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USA.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6338a0-f8e2-42ac-a1c3-b8ba852cb33c",
   "metadata": {},
   "source": [
    "### Check dimensions and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb1456-3dff-4405-a190-3d8b3b544c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions of dfs \n",
    "print(f'SWE \\t   columns: {df_SWE.shape[1]} \\t  rows: {df_SWE.shape[0]}')\n",
    "print(f'FRA \\t   columns: {df_FRA.shape[1]} \\t  rows: {df_FRA.shape[0]}')\n",
    "print(f'ITA \\t   columns: {df_ITA.shape[1]} \\t  rows: {df_ITA.shape[0]}')\n",
    "print(f'USA \\t   columns: {df_USA.shape[1]} \\t  rows: {df_USA.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b1c06-a080-40e8-8abc-f56b5f993404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df_SWE.info() \n",
    "# Most columns are of type 'Dtype object'\n",
    "# Convert columns with strings only to string data type to optimize performance\n",
    "# Mixed columns with both numbers and strings: company_location, salary, job_link (url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa389f-5b82-4bfb-ba60-7c3ecdd57549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert objects to strings \n",
    "df_SWE = df_SWE.astype(dtype_dict)\n",
    "\n",
    "# Output new data types\n",
    "print(df_SWE.dtypes) # Is string[python] not dtype string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183de7c-e9b4-49f2-b0a5-8b4c0d608b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the other datasets \n",
    "df_FRA = df_FRA.astype(dtype_dict)\n",
    "df_ITA = df_ITA.astype(dtype_dict)\n",
    "df_USA = df_USA.astype(dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361ff87-a6c5-4dd5-ba74-99f2c8f025da",
   "metadata": {},
   "source": [
    "## Some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1f6ef-7e1d-4c39-8b46-18d7ff9a2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe (numerical) columns  \n",
    "df_SWE.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96951712-f264-4b05-b487-c3122a251718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe categorical columns  \n",
    "desc_categorical(df_SWE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1492386-7c06-4951-b47a-8c22b68b65f2",
   "metadata": {},
   "source": [
    "### Preliminary observations, Swedish job listings:\n",
    "- __Most frequent job titles__: Data Analyst, systemutvecklare, Data Scientist.\n",
    "- __Most frequent company__: Volvo Group. \n",
    "- __Salary ranges__: Few numerical values are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29476a-d711-4a62-910a-56bc5e7f305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FRA.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7daf27-2c15-4a89-a459-79e1193772e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_categorical(df_FRA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8039089-c7a4-4143-93dc-b9c86ab6bb79",
   "metadata": {},
   "source": [
    "### Preliminary observations, French job listings:\n",
    "- Many listings compared to Sweden. \n",
    "- __Most frequent job titles__: Data Analyst H/F (many similar names for this title, e.g. Data Analyst, Data Analyst F/H). \n",
    "- __Most frequently mentioned company__: AXA.\n",
    "- __Company location__: Includes some information about télétravail (especially common in Paris).  \n",
    "- __Salary ranges__: Need to split column to be able to draw any conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac553b24-c0b8-4827-92b0-5d901326569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ITA.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69189b2-fc06-4217-88fb-cad8d87fd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_categorical(df_ITA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c7c6e-2565-43b1-ae77-41b3432f80ec",
   "metadata": {},
   "source": [
    "### Preliminary observations, Italian job listings:\n",
    "- Many listings compared to Sweden, but less than France.\n",
    "- __Most frequent job titles__: Data Analyst, Data Scientist, Product Analyst. \n",
    "- __Most frequently mentioned company__: BIP - Business Integration Partners. \n",
    "- __Company location__: Milano. Also remote in Milano is relatively common (as is Rome).\n",
    "- __Salary ranges__: Very few numerical entries provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ce1f6-a57e-4384-85d9-4e208f87df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USA.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f1349-1c45-4661-8269-73cfca8137bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_categorical(df_USA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee8816-bc88-44f0-bc1f-d84f20a66e2e",
   "metadata": {},
   "source": [
    "### Preliminary observations, American job listings:\n",
    "- Number of listings are less than for France. \n",
    "- __Most frequent job titles__: Data Analyst, Data Scientist, BI analyst. \n",
    "- __Most frequently mentioned company__: Citi. \n",
    "- __Company location__: New York.  \n",
    "- __Salary ranges__: Need to split column to draw conclusions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde4a4f-a0a7-431b-b6bc-4c16baad1d10",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "- The following columns needs to be cleaned: search_keyword, job_title, job_description, company_location.   \n",
    "- The salary column should be split into two columns (separate numeric vs string content).    \n",
    "- Job titles appear to vary somewhat between countries (since top 3 ones were different for different countries). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08246d4-42ce-4439-94f5-7bd6730252ba",
   "metadata": {},
   "source": [
    "## Data reduction and data cleaning\n",
    "Handle missing and duplicate data entries. Remove unnecessary columns (if any). \n",
    "Clean and preprocess the data to handle anomalies and outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cec211-97b3-4b8f-a75e-07fb383e3cbe",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f30dda-6803-470b-920d-3d8f0949c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values \n",
    "# The scraper labels cells as 'Not available' when there is no value. Change these to NaN. \n",
    "df_SWE.replace('Not available', np.NaN, inplace=True)\n",
    "df_FRA.replace('Not available', np.NaN, inplace=True) \n",
    "df_ITA.replace('Not available', np.NaN, inplace=True)\n",
    "df_USA.replace('Not available', np.NaN, inplace=True)\n",
    "\n",
    "df_FRA.isnull().sum() # Missing salary and job description entries \n",
    "df_SWE.isnull().sum() # Missing salary entries \n",
    "df_ITA.isnull().sum() # Missing salary entries \n",
    "df_USA.isnull().sum() # Missing salary entries \n",
    "\n",
    "# Calculate percentage of missing values\n",
    "df_FRA.isnull().mean() * 100\n",
    "\n",
    "# Salary entries will be examined later when columns are split \n",
    "# For now we do not drop these rows (we are not primarily interested in salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0c2a3-4e4e-43e6-bf49-cdae958f9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns using missingno\n",
    "msno.matrix(df_FRA) #.sample(250)\n",
    "plt.title('Matrix Plot of Missing Values - French listings', fontsize=16)  # Add a title to the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccd47c-e1bc-4551-a7d1-7d7fff6eb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the missing descriptions for French job listings (seems that requests were blocked here?)\n",
    "print(df_FRA.isnull().sum())\n",
    "#df_FRA[df_FRA['job_description'].isnull()]\n",
    "\n",
    "# Manual imports of missing job descriptions from URLs \n",
    "df_FRA_missing = pd.read_csv('df_FRA_missing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19524f1-b6fa-468b-8a17-9a98113b1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the 'job_link' column\n",
    "merged_df = pd.merge(df_FRA, df_FRA_missing, on=['job_link'])\n",
    "merged_df = pd.merge(df_FRA, df_FRA_missing[['job_link', 'job_description_new']], on='job_link', how='left')\n",
    "# Save the job description as only one column (based on when it is not NaN)\n",
    "merged_df['job_description'] = merged_df['job_description'].combine_first(merged_df['job_description_new'])\n",
    "merged_df.drop('job_description_new', axis=1, inplace=True)\n",
    "# Verify that there are no missing values in job_description now\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b647f8c-a47b-44c4-bb88-236d82c43ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()\n",
    "df_FRA = merged_df\n",
    "print(df_FRA.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baba1c6-eaf4-4a40-ba67-7dabc661c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the job_description should not have missing values anymore\n",
    "msno.matrix(df_FRA) #.sample(250)\n",
    "plt.title('Matrix Plot of Missing Values - French listings', fontsize=16)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557040ef-d985-42e2-b43a-136a247c8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are still many missing values for salary, but this is since that information was not always provided in the job listings\n",
    "df_FRA[df_FRA['salary'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9f3d6-39f7-4842-8377-424f0894fd74",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc90f2-ad1b-4ab1-b97a-534be94390cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential duplicates \n",
    "check_duplicates(df_FRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9670ad0-0a6e-4f2e-ae20-47fbcc2a4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FRA.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de705497-adc2-4b3a-963e-63ac0938be8e",
   "metadata": {},
   "source": [
    "__Conclusion:__ There seem to be no issues with duplicated entries for any of the countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf40e71-d3e0-4685-82be-bfa9f8277c5b",
   "metadata": {},
   "source": [
    "## Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8cd21-4fec-4c0c-9beb-cdb9817c599e",
   "metadata": {},
   "source": [
    "### Retrieve numeric values for salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513643a2-0142-44f3-9536-695f5e88223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of data frames \n",
    "dfs = {'SWE': df_SWE, 'FRA': df_FRA, 'ITA': df_ITA, 'USA': df_USA}\n",
    "\n",
    "# Loop through the dictionary and apply the function to clean dfs and add currency and salaries\n",
    "dfs = {key: clean_and_add_currency_and_salaries(df, currency_mapping[key]) for key, df in dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557a8f7-b82a-4f15-b1ca-b3edc5e0a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Unpack the cleaned DataFrames\n",
    "df_SWE, df_FRA, df_ITA, df_USA = dfs.values()\n",
    "\n",
    "# Inspect output \n",
    "print(df_ITA.isnull().sum())\n",
    "# Check salary ranges \n",
    "df_ITA.describe()\n",
    "# Filter out data when column 'salary_num' is not NaN\n",
    "df_filtered = df_ITA.dropna(subset=['salary_num'])\n",
    "df_filtered\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f5251-c7fa-417c-98e7-bf06f6969d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dfs in the dictionary into a single df\n",
    "df_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add dates when data was scraped \n",
    "df_combined['date'] = np.where(df_combined['country'].isin(['Sweden', 'USA']), pd.to_datetime('2024-09-19'), pd.to_datetime('2024-09-20'))\n",
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af582506-cfe4-4efc-a5f0-2091580bc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe good to assign a job id to a column, instead of checking for unique URLs \n",
    "df_combined.insert(0, 'job_id', range(1, len(df_combined) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024835b-b4b5-42f3-801e-730e165ee2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d8800-e259-4fd6-8038-93c489962872",
   "metadata": {},
   "source": [
    "### Detect keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329782e-2afb-4722-b84f-ecf8e7c79fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords \n",
    "common_keywords_SWE = extract_keywords(df_combined, 'Sweden', 'swedish')\n",
    "common_keywords_FRA = extract_keywords(df_combined, 'France', 'french')\n",
    "common_keywords_ITA = extract_keywords(df_combined, 'Italy', 'italian')\n",
    "common_keywords_USA = extract_keywords(df_combined, 'USA', 'english')\n",
    "\n",
    "# Plot common keywords \n",
    "plot_common_keywords(common_keywords_SWE[0], 'Sweden')\n",
    "plot_common_keywords(common_keywords_FRA[0], 'France')\n",
    "plot_common_keywords(common_keywords_ITA[0], 'Italy')\n",
    "plot_common_keywords(common_keywords_USA[0], 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2ff89-586d-44a7-b48e-c3d65d947add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate software/programming keyword counts for each country \n",
    "keyword_counts_SWE = count_keywords(df_combined, 'Sweden')\n",
    "keyword_counts_FRA = count_keywords(df_combined, 'France')\n",
    "keyword_counts_ITA = count_keywords(df_combined, 'Italy')\n",
    "keyword_counts_USA = count_keywords(df_combined, 'USA')\n",
    "\n",
    "# Combine all dfs into one\n",
    "keyword_counts_combined = pd.concat([keyword_counts_SWE, keyword_counts_FRA, keyword_counts_ITA, keyword_counts_USA], ignore_index=True)\n",
    "keyword_counts_combined.sort_values(by=['Count'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba0cf8-4475-427d-8c8a-46a51bce68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract interview information\n",
    "interview_info_df = extract_interview_info(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1748772-c1c6-422e-88f2-e18523caf0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total occurrences of each interview stage\n",
    "interview_stage_counts = interview_info_df.sum()\n",
    "\n",
    "# Convert the Series to a DataFrame for better readability\n",
    "interview_stage_counts_df = interview_stage_counts.reset_index()\n",
    "interview_stage_counts_df.columns = ['Interview Stage', 'Count'].sort?\n",
    "\n",
    "print(interview_stage_counts_df)\n",
    "\n",
    "# Many presentations: could this perhaps be that there are a lot of the jobs that include 'presentations' as a job assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3320411-409d-4d37-b3ce-3e6f0e23892e",
   "metadata": {},
   "source": [
    "## Univariate analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922b5c-b1f5-481a-90a7-a366b465fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms for salaries \n",
    "# to do "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1e624-fc33-47bb-8da2-bbe7e7b66e65",
   "metadata": {},
   "source": [
    "## Bivariate analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab246a3f-2ebe-4593-b606-b91bf04d8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud visualizations per country\n",
    "def plt_wordtree(data, country):\n",
    "    #df = data['job_description']\n",
    "    #country = 'France'\n",
    "    # Combine all the text into a single string \n",
    "    text = ' '.join(data)\n",
    "    # Create a wordcloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    # Display the wordcloud \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Wordcloud of Job Descriptions - {country}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plt_wordtree(common_keywords_SWE[1], 'Sweden')\n",
    "plt_wordtree(common_keywords_FRA[1], 'France')\n",
    "plt_wordtree(common_keywords_ITA[1], 'Italy')\n",
    "plt_wordtree(common_keywords_USA[1], 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e63905-4ee5-4580-b402-28c1a098cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add also word cloud visualizations per job title? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae366f7-cbf4-4cd9-876a-fa24be705b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots help visualize the distribution of salary ranges across different categories (e.g., job titles, countries).\n",
    "# Look at outliers\n",
    "\n",
    "# Number of job listings per job title and country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df44d7d-3801-4630-8d12-a470437ab621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note: salaries are not in the same currency so comparisons cant really be made \n",
    "\n",
    "sns.boxplot(data=df_combined, x='search_keyword', y='salary_num_low')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Salary Distribution by Job Title')\n",
    "plt.show()\n",
    "\n",
    "mean_salary = df_combined.groupby('search_keyword')['salary_num_low'].mean().reset_index()\n",
    "sns.barplot(data=mean_salary, x='search_keyword', y='salary_num_low')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Average Salary by Job Title')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=df_combined, x='search_keyword', hue='salary_num_low')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Count of Job Titles by Salary Range')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=df_combined, x='search_keyword', hue='country')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Count of Job Titles by Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648065f1-0b50-4642-b3b0-f02f5e01b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "salary_ranges = pd.cut(df_combined['salary_num_low'], bins=[0, 20000, 40000, 60000, 80000, 100000], labels=['<20k', '20-40k', '40-60k', '60-80k', '80-100k'])\n",
    "crosstab = pd.crosstab(df_combined['search_keyword'], salary_ranges)\n",
    "chi2, p, dof, expected = stats.chi2_contingency(crosstab)\n",
    "print(f'Chi-squared: {chi2}, p-value: {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18dfe9-fae9-48a0-986e-1c2463811543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = df_combined.pivot_table(values='salary_num_low', index='country', columns='search_keyword', aggfunc='mean')\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu')\n",
    "plt.title('Average Salary Heatmap by Country and Job Title')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "g = sns.FacetGrid(df_clean, col='search_location', col_wrap=3)\n",
    "g.map(sns.boxplot, 'search_keyword', 'salary_num_low')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5738d-42e7-462a-a129-343953578ab9",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "TBA.\n",
    "Also interesting to look into 'Recruitment process' and 'Interview' process since there is data about that in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d13f98-8d74-4093-b01b-a51c7b890d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
