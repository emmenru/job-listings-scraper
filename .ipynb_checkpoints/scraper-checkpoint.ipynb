{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaf65a-cd7c-4c50-aa5c-450e75643b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages \n",
    "#!pip install beautifulsoup4 lxml selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3c9e26e-5774-4eff-b3df-22d752103b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import time\n",
    "import requests\n",
    "from csv import writer\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree as et\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0fba3dd-183d-4e56-bb44-0d609c741104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define job and location search keywords\n",
    "job_search_keyword = ['Data+Analyst']#, 'Data+Scientist', 'Product+Analyst', 'BI+Analyst']\n",
    "location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']\n",
    "\n",
    "# define base and pagination URLs\n",
    "base_url = 'https://www.indeed.com'\n",
    "paginaton_url = \"https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}\"\n",
    "# use these more consistently below\n",
    "\n",
    "# check also Sweden and France \n",
    "# Just pick the 3 largest cities in each country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcde12ed-f6f2-4e9b-b707-f22f81a8f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get DOM from given URL\n",
    "def get_dom(url):\n",
    "    driver.get(url)\n",
    "    #WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"jobDescriptionText\")))\n",
    "    time.sleep(3)  # ensure page loads\n",
    "    page_content = driver.page_source\n",
    "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    dom = et.HTML(str(product_soup))\n",
    "    return dom\n",
    "\n",
    "# function to extract job link\n",
    "def get_job_link(job):\n",
    "    try:\n",
    "        return job.xpath('./descendant::h2/a/@href')[0]\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "# function to extract job description\n",
    "def get_job_desc(job_link):\n",
    "    job_dom = get_dom(job_link)\n",
    "    try:\n",
    "        job_desc = job_dom.xpath('//*[@id=\"jobDescriptionText\"]//text()')\n",
    "        return \" \".join(job_desc).strip() if job_desc else 'Not available'\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "# function to extract job title\n",
    "def get_job_title(job):\n",
    "   try:\n",
    "       job_title = job.xpath('./descendant::h2/a/span/text()')[0]\n",
    "   except Exception as e:\n",
    "       job_title = 'Not available'\n",
    "   return job_title\n",
    "\n",
    "# function to extract  company name\n",
    "def get_company_name(job):\n",
    "   try:\n",
    "       company_name = job.xpath('.//span[@data-testid=\"company-name\"]/text()')[0]\n",
    "       #company_name = job.xpath('./descendant::span[@class=\"companyName\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_name = 'Not available'\n",
    "   return company_name\n",
    "\n",
    "# function to extract company location\n",
    "def get_company_location(job):\n",
    "   try:\n",
    "       company_location = job.xpath('.//div[@data-testid=\"text-location\"]/text()')[0]\n",
    "       #company_location = job.xpath('./descendant::div[@class=\"companyLocation\"]/text()')[0]\n",
    "   except Exception as e:\n",
    "       company_location = 'Not available'\n",
    "   return company_location\n",
    "\n",
    "# function to extract salary info\n",
    "def get_salary(job_link):\n",
    "    job_dom = get_dom(job_link)\n",
    "    try:\n",
    "        # Use the provided XPath to get the salary text\n",
    "        salary = job_dom.xpath('//*[@id=\"salaryInfoAndJobType\"]//text()')\n",
    "        return \" \".join(salary).strip() if salary else 'Not available'\n",
    "    except Exception:\n",
    "        return 'Not available'\n",
    "\n",
    "# function to estimate number of pages for a specific search\n",
    "def get_total_pages(job_keyword, location_keyword):\n",
    "    # url of Indeed job search\n",
    "    url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}\"\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        # wait for the element containing the job count to appear\n",
    "        job_count_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, \"jobsearch-JobCountAndSortPane-jobCount\")]'))\n",
    "        )\n",
    "        # extract the text from the element\n",
    "        job_count_text = job_count_element.text\n",
    "        print(f\"Job count text: {job_count_text}\")\n",
    "        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces\n",
    "        # each page shows approx. 15 jobs\n",
    "        jobs_per_page = 15\n",
    "        # calculate the total number of pages\n",
    "        total_pages = math.ceil(job_count / jobs_per_page)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting job count: {e}\")\n",
    "        return 0  # Return 0 if there's an error\n",
    "    # is this really needed?\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        #driver.quit()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5dc8a2a-339f-4938-baf3-5133bdd6de9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: Data Scientist in New York\n",
      "Error extracting job count: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100eeb338 chromedriver + 5096248\n",
      "1   chromedriver                        0x0000000100ee2b6a chromedriver + 5061482\n",
      "2   chromedriver                        0x0000000100a6dfd0 chromedriver + 389072\n",
      "3   chromedriver                        0x0000000100aba9ab chromedriver + 702891\n",
      "4   chromedriver                        0x0000000100abaa81 chromedriver + 703105\n",
      "5   chromedriver                        0x0000000100afecd4 chromedriver + 982228\n",
      "6   chromedriver                        0x0000000100adef1d chromedriver + 851741\n",
      "7   chromedriver                        0x0000000100afc250 chromedriver + 971344\n",
      "8   chromedriver                        0x0000000100adec93 chromedriver + 851091\n",
      "9   chromedriver                        0x0000000100aadc79 chromedriver + 650361\n",
      "10  chromedriver                        0x0000000100aae49e chromedriver + 652446\n",
      "11  chromedriver                        0x0000000100eae0b0 chromedriver + 4845744\n",
      "12  chromedriver                        0x0000000100eb2fc8 chromedriver + 4865992\n",
      "13  chromedriver                        0x0000000100eb3695 chromedriver + 4867733\n",
      "14  chromedriver                        0x0000000100e90ce9 chromedriver + 4725993\n",
      "15  chromedriver                        0x0000000100eb3989 chromedriver + 4868489\n",
      "16  chromedriver                        0x0000000100e82c04 chromedriver + 4668420\n",
      "17  chromedriver                        0x0000000100ed2e68 chromedriver + 4996712\n",
      "18  chromedriver                        0x0000000100ed3067 chromedriver + 4997223\n",
      "19  chromedriver                        0x0000000100ee276e chromedriver + 5060462\n",
      "20  libsystem_pthread.dylib             0x00007fff6f86b109 _pthread_start + 148\n",
      "21  libsystem_pthread.dylib             0x00007fff6f866b8b thread_start + 15\n",
      "\n",
      "Total pages found: 0\n",
      "Fetching Page number: 1\n",
      "Jobs found on page 1: 0\n",
      "Fetching Page number: 2\n",
      "Jobs found on page 2: 0\n"
     ]
    }
   ],
   "source": [
    "# initialize Chrome webdriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# open URL\n",
    "driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
    "\n",
    "# open a CSV file to write the job listings data\n",
    "with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    theWriter = writer(f)\n",
    "    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']\n",
    "    theWriter.writerow(heading)\n",
    "\n",
    "    for job_keyword in job_search_keywords:\n",
    "        for location_keyword in location_search_keywords:\n",
    "            print(f\"Searching for: {job_keyword} in {location_keyword}\")\n",
    "            all_jobs = []\n",
    "\n",
    "            # find total number of pages\n",
    "            total_pages = get_total_pages(job_keyword, location_keyword)\n",
    "            print(f\"Total pages found: {total_pages}\") \n",
    "\n",
    "            for page_no in range(total_pages):  # Modify range to get more or less pages, if needed\n",
    "                print(f\"Fetching Page number: {page_no + 1}\")  # display page number (1-based)\n",
    "                url = f\"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}\"\n",
    "                page_dom = get_dom(url)\n",
    "                \n",
    "                # extract jobs from current page\n",
    "                jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
    "                print(f\"Jobs found on page {page_no + 1}: {len(jobs)}\") \n",
    "                \n",
    "                # process each job\n",
    "                for job in jobs:\n",
    "                    job_link = base_url + get_job_link(job)\n",
    "                    job_title = get_job_title(job)\n",
    "                    company_name = get_company_name(job)\n",
    "                    company_location = get_company_location(job)\n",
    "                    salary = get_salary(job_link)\n",
    "                    job_desc = get_job_desc(job_link)  # extract job description \n",
    "                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]\n",
    "                    print(page_no + 1, job_link)  # print page number (1-based index) and job link\n",
    "                    theWriter.writerow(record)  # write the record to CSV\n",
    "\n",
    "# close the web browser\n",
    "driver.quit()\n",
    "\n",
    "# save prints as log file if connection goes down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7186d2-5422-4352-a4af-6ad96bfeda22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
