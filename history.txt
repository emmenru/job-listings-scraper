# import necessary modules
import time
import requests
from csv import writer
from bs4 import BeautifulSoup
from lxml import etree as et
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
# define job and location search keywords
job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']
location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']

# define base and pagination URLs
base_url = 'https://www.indeed.com'
paginaton_url = "https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}"

# check also Sweden and France 
# Just pick the 3 largest cities in each country
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'
        
# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    # Example job search keywords and locations
    job_search_keywords = ['Data Scientist']  # Replace with actual keywords
    location_search_keywords = ['New York']  # Replace with actual locations

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_type = get_job_type(job)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    # Example job search keywords and locations
    job_search_keywords = ['Data Scientist']  # Replace with actual keywords
    location_search_keywords = ['New York']  # Replace with actual locations

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
def get_total_pages(url):
    page_dom = get_dom(url)  # Get the DOM of the search results page
    try:
        # Adjust the XPath according to the actual HTML structure of the pagination
        total_pages_text = page_dom.xpath('//div[@class="pagination"]/a[last()]/text()')[0]
        total_pages = int(total_pages_text.split()[-1])  # Extract the last number
        return total_pages
    except Exception as e:
        print(f"Error extracting total pages: {e}")
        return 1  # Return 1 if there's an error, assuming at least one page

# Example usage
search_url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York"
total_pages = get_total_pages(search_url)
print(f"Total pages found: {total_pages}")
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

def get_total_pages(url):
    page_dom = get_dom(url)  # Get the DOM of the search results page
    try:
        # Adjust the XPath according to the actual HTML structure of the pagination
        total_pages = page_dom.xpath('//ul[@class="pagination-list"]/li[last()]/a/text()')
        return int(total_pages[0].strip()) if total_pages else 1  # Extract total number of pages
    except Exception as e:
        print(f"Error extracting total pages: {e}")
        return 1  # Default to 1 page if there's an error or no pagination
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = get_total_pages(first_page_url)
            print(f"Total pages found: {total_pages}")
            
            all_jobs = []
            for page_no in range(total_pages):  # Loop through all pages
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link

                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

def get_total_pages(url):
    # Load the DOM of the search results page
    page_dom = get_dom(url)
    try:
        # XPath for extracting the number of pages (this is often located in the pagination section)
        # Adjusted to fit Indeed's structure
        total_pages = page_dom.xpath('//ul[@class="pagination-list"]/li[last()-1]/a/text()')
        
        # Ensure that the result is converted to an integer and handle cases where no pagination is found
        return int(total_pages[0].strip()) if total_pages else 1
    except Exception as e:
        print(f"Error extracting total pages: {e}")
        return 1  # Default to 1 page if pagination is not available or any error occurs
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = get_total_pages(first_page_url)
            print(f"Total pages found: {total_pages}")
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

def get_total_pages(url):
    # Load the DOM of the search results page
    page_dom = get_dom(url)
    try:
        # XPath for extracting the number of pages (this is often located in the pagination section)
        # Adjusted to fit Indeed's structure
        total_pages = page_dom.xpath('//*[@id="jobsearch-JapanPage"]/div/div[5]/div/div[1]/div[3]/div/div/div[2]/span[1]/text()')
        
        # Ensure that the result is converted to an integer and handle cases where no pagination is found
        return int(total_pages[0].strip()) if total_pages else 1
    except Exception as e:
        print(f"Error extracting total pages: {e}")
        return 1  # Default to 1 page if pagination is not available or any error occurs
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = get_total_pages(first_page_url)
            print(f"Total pages found: {total_pages}")
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = driver.find_element(By.XPATH, '//*[@id="searchCountPages"]').text
            print(f"Total pages found: {total_pages}")
driver.quit()
from selenium.webdriver.common.by import By  # Add this import for 'By'

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = driver.find_element(By.XPATH, '//*[@id="searchCountPages"]').text
            print(f"Total pages found: {total_pages}")
driver.quit()
from selenium.webdriver.common.by import By  # Add this import for 'By'

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = driver.find_element(By.XPATH, '//*[@id="searchCountPages"]').text
            print(f"Total pages found: {total_pages}")
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            
            # Step 1: Get the first page URL for this search
            first_page_url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
            
            # Step 2: Get the total number of pages
            total_pages = 300
            print(f"Total pages found: {total_pages}")
            
            all_jobs = []
            for page_no in range(total_pages):  # Loop through all pages
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link

                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the total job count to appear and extract it
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[@id="searchCountPages"]'))
    )
    
    # Extract text and parse the total number of jobs
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    
    # Example of extracting the total number of jobs from the string
    # e.g. "Page 1 of 17 jobs" or similar
    total_jobs = int(job_count_text.split()[-2].replace(",", ""))  # Removes commas if any
    print(f"Total jobs found: {total_jobs}")

    # Each page shows 15 jobs (you can adjust this if the number per page differs)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(total_jobs / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    
    # Extract the total number of jobs using string splitting and cleanup
    total_jobs = int(job_count_text.split()[3].replace(",", ""))  # Adjust splitting based on the actual text format
    print(f"Total jobs found: {total_jobs}")

    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(total_jobs / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    
    # Extract the total number of jobs using string splitting and cleanup
    total_jobs = int(job_count_text.split()[3].replace("+", ",", ""))  # Adjust splitting based on the actual text format
    print(f"Total jobs found: {total_jobs}")

    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(total_jobs / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = job_count_text.split('+')
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = job_count_text.split('+')[0]
    print(job_count)
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = job_count_text.split('+')[0]
    print(int(job_count))
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = "https://www.indeed.com/jobs?q=Data+Scientist&l=New+York&from=searchOnDesktopSerp&vjk=d1b96df5f19292fd"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
# import necessary modules
import time
import requests
from csv import writer
from bs4 import BeautifulSoup
from lxml import etree as et
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import math
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, , job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit()
import math
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from csv import writer

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        driver.quit()

# Main script
job_search_keywords = ["Data Scientist"]  # Example job keyword
location_search_keywords = ["New York"]  # Example location

# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # Get total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)

            for page_no in range(total_pages):  # Use total pages for the loop
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from csv import writer

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)

            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # Initialize Chrome webdriver
    #service = Service(ChromeDriverManager().install())
    #driver = webdriver.Chrome(service=service)

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # Initialize Chrome webdriver
    #service = Service(ChromeDriverManager().install())
    #driver = webdriver.Chrome(service=service)

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(3):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(3):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(2):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):
    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # Find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            print(f"Total pages found: {total_pages}")  # Print total pages

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                
                # Extract jobs from current page
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # Find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            print(f"Total pages found: {total_pages}")  # Print total pages

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                
                # Extract jobs from current page
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
                
                # Process each job
                for job in jobs:
                    job_link = base_url + get_job_link(job)
                    job_title = get_job_title(job)
                    company_name = get_company_name(job)
                    company_location = get_company_location(job)
                    salary = get_salary(job_link)
                    job_desc = get_job_desc(job_link)  # Extract job description for this job

                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                    theWriter.writerow(record)  # Write the record to CSV

# Closing the web browser
driver.quit()
# write code here to automate saving for different countries, cities, and job entries, and so on 
def scrape_indeed_jobs(driver_link, search_keyword, search_location):
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)

    # Open CSV file to write job listings data
    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:
        theWriter = writer(f)
        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
        theWriter.writerow(heading)

        # Find total number of pages
        total_pages = get_total_pages(search_keyword, search_location)
        print(f"Total pages found: {total_pages}")  # Print total pages

        for page_no in range(total_pages):  # Modify range to get more pages if needed
            print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
            url = f"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}"
            page_dom = get_dom(url)
            
            # Extract jobs from current page
            jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
            print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
            
            # Process each job
            for job in jobs:
                job_link = base_url + get_job_link(job)
                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description for this job

                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]

                print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                theWriter.writerow(record)  # Write the record to CSV

    # Closing the web browser
    driver.quit()

# Example of calling the function
scrape_indeed_jobs("https://www.indeed.com/q-USA-jobs.html", "Data+Scientist", "New+York")
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # Find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            print(f"Total pages found: {total_pages}")  # Print total pages

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                
                # Extract jobs from current page
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
                
                # Process each job
                for job in jobs:
                    job_link = base_url + get_job_link(job)
                    job_title = get_job_title(job)
                    company_name = get_company_name(job)
                    company_location = get_company_location(job)
                    salary = get_salary(job_link)
                    job_desc = get_job_desc(job_link)  # Extract job description for this job

                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                    theWriter.writerow(record)  # Write the record to CSV

# Closing the web browser
driver.quit()
def scrape_jobs():
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)
    
    # Open URL
    driver.get("https://www.indeed.com/q-USA-jobs.html")
    
    # Open a CSV file to write the job listings data
    with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
        theWriter = writer(f)
        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
        theWriter.writerow(heading)
    
        for job_keyword in job_search_keywords:
            for location_keyword in location_search_keywords:
                print(f"Searching for: {job_keyword} in {location_keyword}")
                all_jobs = []
    
                # Find total number of pages
                total_pages = get_total_pages(job_keyword, location_keyword)
                print(f"Total pages found: {total_pages}")  # Print total pages
    
                for page_no in range(total_pages):  # Modify range to get more pages if needed
                    print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
                    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                    page_dom = get_dom(url)
                    
                    # Extract jobs from current page
                    jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                    print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
                    
                    # Process each job
                    for job in jobs:
                        job_link = base_url + get_job_link(job)
                        job_title = get_job_title(job)
                        company_name = get_company_name(job)
                        company_location = get_company_location(job)
                        salary = get_salary(job_link)
                        job_desc = get_job_desc(job_link)  # Extract job description for this job
    
                        record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]
    
                        print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                        theWriter.writerow(record)  # Write the record to CSV
    
    # Closing the web browser
    driver.quit()

scrape_jobs()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # Find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            print(f"Total pages found: {total_pages}")  # Print total pages

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                
                # Extract jobs from current page
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
                
                # Process each job
                for job in jobs:
                    job_link = base_url + get_job_link(job)
                    job_title = get_job_title(job)
                    company_name = get_company_name(job)
                    company_location = get_company_location(job)
                    salary = get_salary(job_link)
                    job_desc = get_job_desc(job_link)  # Extract job description for this job

                    record = [page_no + 1, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                    print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                    theWriter.writerow(record)  # Write the record to CSV

# Closing the web browser
driver.quit()
# write code here to automate saving for different countries, cities, and job entries, and so on 

# Modify get_total_pages and get_dom functions to accept driver as a parameter
def get_total_pages(job_keyword, location_keyword, driver):
    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        job_count_text = job_count_element.text
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces
        jobs_per_page = 15
        total_pages = math.ceil(job_count / jobs_per_page)

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

def get_dom(url, driver):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

def scrape_indeed_jobs(search_keyword, search_location):
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)

    # Open CSV file to write job listings data
    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:
        theWriter = writer(f)
        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
        theWriter.writerow(heading)

        # Find total number of pages
        total_pages = get_total_pages(search_keyword, search_location, driver)
        print(f"Total pages found: {total_pages}")  # Print total pages

        for page_no in range(total_pages):  # Modify range to get more pages if needed
            print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
            url = f"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}"
            page_dom = get_dom(url, driver)
            
            # Extract jobs from current page
            jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
            print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
            
            # Process each job
            for job in jobs:
                job_link = base_url + get_job_link(job)
                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description for this job

                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]

                print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                theWriter.writerow(record)  # Write the record to CSV

    # Closing the web browser
    driver.quit()

# Example of calling the function
scrape_indeed_jobs("Data+Scientist", "New+York")
def scrape_indeed_jobs(search_keyword, search_location):
    # Initialize Chrome webdriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)

    # Open CSV file to write job listings data
    with open('indeed_jobs.csv', 'w', newline='', encoding='utf-8') as f:
        theWriter = writer(f)
        heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
        theWriter.writerow(heading)

        # Find total number of pages
        total_pages = get_total_pages(search_keyword, search_location, driver)
        print(f"Total pages found: {total_pages}")  # Print total pages

        for page_no in range(total_pages):  # Modify range to get more pages if needed
            print(f"Fetching Page number: {page_no + 1}")  # Display page number (1-based)
            url = f"https://www.indeed.com/jobs?q={search_keyword}&l={search_location}&start={page_no * 10}"
            page_dom = get_dom(url, driver)
            
            # Extract jobs from current page
            jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
            print(f"Jobs found on page {page_no + 1}: {len(jobs)}")  # Print number of jobs found
            
            # Process each job
            for job in jobs:
                job_link = base_url + get_job_link(job)
                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link, driver)  # Pass the driver
                job_desc = get_job_desc(job_link, driver)  # Pass the driver

                record = [page_no + 1, job_link, search_keyword, search_location, job_title, company_name, company_location, salary, job_desc]

                print(page_no + 1, job_link)  # Print page number (1-based index) and job link
                theWriter.writerow(record)  # Write the record to CSV

    # Closing the web browser
    driver.quit()

def get_salary(job_link, driver):  # Accept driver as an argument
    job_dom = get_dom(job_link, driver)  # Pass the driver
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return ''.join(salary).strip() if salary else "Not specified"
    except Exception as e:
        print(f"Error getting salary for {job_link}: {e}")
        return "Error"

def get_job_desc(job_link, driver):  # Accept driver as an argument
    job_dom = get_dom(job_link, driver)  # Pass the driver
    # Extract job description logic goes here
    # Example:
    try:
        job_desc = job_dom.xpath('//div[@class="jobsearch-jobDescriptionText"]//text()')
        return ''.join(job_desc).strip() if job_desc else "No description available"
    except Exception as e:
        print(f"Error getting job description for {job_link}: {e}")
        return "Error"

# Example of calling the function
scrape_indeed_jobs("Data+Scientist", "New+York")
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)
                # are these all jobs or just all the jobs for one page? 

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit().
# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)
                # are these all jobs or just all the jobs for one page? 

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# import necessary modules
import time
import math
import requests
from csv import writer
from bs4 import BeautifulSoup
from lxml import etree as et
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# define job and location search keywords
job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst'] # check also Sweden and France 
location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago'] # Just pick the 3 largest cities in each country 

# define base and pagination URLs
base_url = 'https://www.indeed.com'
paginaton_url = "https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}"
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

# Function to extract job title
def get_job_title(job):
    try:
        job_title = job.xpath('./descendant::h2/a/span/text()')[0]
    except Exception:
        job_title = 'Not available'
    return job_title

# Function to extract the company name
def get_company_name(job):
    try:
        company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
    except Exception:
        company_name = 'Not available'
    return company_name

# Function to extract the company location
def get_company_location(job):
    try:
        company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
    except Exception:
        company_location = 'Not available'
    return company_location

# Function to extract salary
def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'

# Function to get total number of pages
def get_total_pages(job_keyword, location_keyword):

    # URL of Indeed job search
    url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
    driver.get(url)

    try:
        # Wait for the element containing the job count to appear
        job_count_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
        )
        
        # Extract the text from the element
        job_count_text = job_count_element.text
        print(f"Job count text: {job_count_text}")
        job_count = int(job_count_text.split('+')[0].replace(',', '').strip())  # Handle commas and extra spaces

        # Each page shows 15 jobs
        jobs_per_page = 15

        # Calculate the total number of pages
        total_pages = math.ceil(job_count / jobs_per_page)
        print(f"Total pages: {total_pages}")

        return total_pages

    except Exception as e:
        print(f"Error extracting job count: {e}")
        return 0  # Return 0 if there's an error

    finally:
        # Close the browser
        #driver.quit()
        pass
# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)
                # are these all jobs or just all the jobs for one page? 

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()

driver = webdriver.Chrome(ChromeDriverManager().install())

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['page', 'job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            total_pages = get_total_pages(job_keyword, location_keyword)
            # do we really want this to start the driver and stop it

            for page_no in range(total_pages):  # Modify range to get more pages if needed
                print(f"Page number: {page_no}")
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)
                # are these all jobs or just all the jobs for one page? 

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [page_no, job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(page_no, job_link)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# import necessary modules
import time
import requests
from csv import writer
from bs4 import BeautifulSoup
from lxml import etree as et
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
# define job and location search keywords
job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']
location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']

# define base and pagination URLs
base_url = 'https://www.indeed.com'
paginaton_url = "https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}"

# check also Sweden and France 
# Just pick the 3 largest cities in each country
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

def get_skills(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use a refined XPath to get all relevant text within the profile insights
        skills = job_dom.xpath('//div[contains(@class, "profile-insights")]//text()')
        return " ".join(skills).strip() if skills else 'Not available'
    except Exception:
        return 'Not available'

# functions to extract job title
def get_job_title(job):
   try:
       job_title = job.xpath('./descendant::h2/a/span/text()')[0]
   except Exception as e:
       job_title = 'Not available'
   return job_title

# functions to extract the company name
def get_company_name(job):
   try:
       company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
       #company_name = job.xpath('./descendant::span[@class="companyName"]/text()')[0]
   except Exception as e:
       company_name = 'Not available'
   return company_name

# functions to extract the company location
def get_company_location(job):
   try:
       company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
       #company_location = job.xpath('./descendant::div[@class="companyLocation"]/text()')[0]
   except Exception as e:
       company_location = 'Not available'
   return company_location

# functions to extract salary information
'''
def get_salary(job):
   try:
       salary = job.xpath('./descendant::span[@class="estimated-salary"]/span/text()')
   except Exception as e:
       salary = 'Not available'
   if len(salary) == 0:
       try:
           salary = job.xpath('./descendant::div[@class="metadata salary-snippet-container"]/div/text()')[0]
       except Exception as e:
           salary = 'Not available'
   else:
       salary = salary[0]
   return salary
'''

def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
        
# functions to extract job type
def get_job_type(job):
   try:
       job_type = job.xpath('./descendant::div[@class="metadata"]/div/text()')[0]
   except Exception as e:
       job_type = 'Not available'
   return job_type
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_type', 'job_description']
    theWriter.writerow(heading)

    # Example job search keywords and locations
    job_search_keywords = ['Data Scientist']  # Replace with actual keywords
    location_search_keywords = ['New York']  # Replace with actual locations

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_type = get_job_type(job)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
                skills = get_skills(job_link)  # Extract skills

                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, skills, job_type, job_desc]

                print(salary)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# import necessary modules
import time
import requests
from csv import writer
from bs4 import BeautifulSoup
from lxml import etree as et
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
# define job and location search keywords
job_search_keyword = ['Data+Scientist']#, 'Data+Analyst', 'Product+Analyst', 'BI+Analyst']
location_search_keyword = ['New+York']#, 'Los+Angeles', 'Chicago']

# define base and pagination URLs
base_url = 'https://www.indeed.com'
paginaton_url = "https://www.indeed.com/jobs?q={}&l={}&radius=35&start={}"

# check also Sweden and France 
# Just pick the 3 largest cities in each country
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

def get_skills(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use a refined XPath to get all relevant text within the profile insights
        skills = job_dom.xpath('//div[contains(@class, "profile-insights")]//text()')
        return " ".join(skills).strip() if skills else 'Not available'
    except Exception:
        return 'Not available'

# functions to extract job title
def get_job_title(job):
   try:
       job_title = job.xpath('./descendant::h2/a/span/text()')[0]
   except Exception as e:
       job_title = 'Not available'
   return job_title

# functions to extract the company name
def get_company_name(job):
   try:
       company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
       #company_name = job.xpath('./descendant::span[@class="companyName"]/text()')[0]
   except Exception as e:
       company_name = 'Not available'
   return company_name

# functions to extract the company location
def get_company_location(job):
   try:
       company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
       #company_location = job.xpath('./descendant::div[@class="companyLocation"]/text()')[0]
   except Exception as e:
       company_location = 'Not available'
   return company_location

# functions to extract salary information
'''
def get_salary(job):
   try:
       salary = job.xpath('./descendant::span[@class="estimated-salary"]/span/text()')
   except Exception as e:
       salary = 'Not available'
   if len(salary) == 0:
       try:
           salary = job.xpath('./descendant::div[@class="metadata salary-snippet-container"]/div/text()')[0]
       except Exception as e:
           salary = 'Not available'
   else:
       salary = salary[0]
   return salary
'''

def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
        
# functions to extract job type
def get_job_type(job):
   try:
       job_type = job.xpath('./descendant::div[@class="metadata"]/div/text()')[0]
   except Exception as e:
       job_type = 'Not available'
   return job_type
# Function to get DOM from given URL
def get_dom(url):
    driver.get(url)
    time.sleep(3)  # Ensure page loads
    page_content = driver.page_source
    product_soup = BeautifulSoup(page_content, 'html.parser')
    dom = et.HTML(str(product_soup))
    return dom

# Function to extract job link
def get_job_link(job):
    try:
        return job.xpath('./descendant::h2/a/@href')[0]
    except Exception:
        return 'Not available'

# Function to extract job description
def get_job_desc(job_link):
    job_dom = get_dom(job_link)
    try:
        job_desc = job_dom.xpath('//*[@id="jobDescriptionText"]//text()')
        return " ".join(job_desc).strip() if job_desc else 'Not available'
    except Exception:
        return 'Not available'

def get_skills(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use a refined XPath to get all relevant text within the profile insights
        skills = job_dom.xpath('//div[contains(@class, "profile-insights")]//text()')
        return " ".join(skills).strip() if skills else 'Not available'
    except Exception:
        return 'Not available'

# functions to extract job title
def get_job_title(job):
   try:
       job_title = job.xpath('./descendant::h2/a/span/text()')[0]
   except Exception as e:
       job_title = 'Not available'
   return job_title

# functions to extract the company name
def get_company_name(job):
   try:
       company_name = job.xpath('.//span[@data-testid="company-name"]/text()')[0]
       #company_name = job.xpath('./descendant::span[@class="companyName"]/text()')[0]
   except Exception as e:
       company_name = 'Not available'
   return company_name

# functions to extract the company location
def get_company_location(job):
   try:
       company_location = job.xpath('.//div[@data-testid="text-location"]/text()')[0]
       #company_location = job.xpath('./descendant::div[@class="companyLocation"]/text()')[0]
   except Exception as e:
       company_location = 'Not available'
   return company_location

# functions to extract salary information
'''
def get_salary(job):
   try:
       salary = job.xpath('./descendant::span[@class="estimated-salary"]/span/text()')
   except Exception as e:
       salary = 'Not available'
   if len(salary) == 0:
       try:
           salary = job.xpath('./descendant::div[@class="metadata salary-snippet-container"]/div/text()')[0]
       except Exception as e:
           salary = 'Not available'
   else:
       salary = salary[0]
   return salary
'''

def get_salary(job_link):
    job_dom = get_dom(job_link)
    try:
        # Use the provided XPath to get the salary text
        salary = job_dom.xpath('//*[@id="salaryInfoAndJobType"]//text()')
        return " ".join(salary).strip() if salary else 'Not available'
    except Exception:
        return 'Not available'
        
# functions to extract job type
def get_job_type(job):
   try:
       job_type = job.xpath('./descendant::div[@class="metadata"]/div/text()')[0]
   except Exception as e:
       job_type = 'Not available'
   return job_type
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit() Here is the other code where I would like to incorporate it: # Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
# Initialize Chrome webdriver
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# URL of Indeed job search
url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}"
driver.get(url)

try:
    # Wait for the element containing the job count to appear (using a more flexible XPath)
    job_count_element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//div[contains(@class, "jobsearch-JobCountAndSortPane-jobCount")]'))
    )
    
    # Extract the text from the element
    job_count_text = job_count_element.text
    print(f"Job count text: {job_count_text}")
    job_count = int(job_count_text.split('+')[0])
    
    # Each page shows 15 jobs (Indeed usually shows 15 jobs per page)
    jobs_per_page = 15

    # Calculate the total number of pages
    total_pages = math.ceil(job_count / jobs_per_page)
    print(f"Total pages: {total_pages}")

except Exception as e:
    print(f"Error extracting job count: {e}")

# Close the browser
driver.quit() 
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

# Open URL
driver.get("https://www.indeed.com/q-USA-jobs.html")

# Open a CSV file to write the job listings data
with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:
    theWriter = writer(f)
    heading = ['job_link', 'search_keyword', 'search_location', 'job_title', 'company_name', 'company_location', 'salary', 'skills', 'job_description']
    theWriter.writerow(heading)

    for job_keyword in job_search_keywords:
        for location_keyword in location_search_keywords:
            print(f"Searching for: {job_keyword} in {location_keyword}")
            all_jobs = []

            # find total number of pages
            
            for page_no in range(2):  # Modify range to get more pages if needed
                url = f"https://www.indeed.com/jobs?q={job_keyword}&l={location_keyword}&start={page_no * 10}"
                page_dom = get_dom(url)
                jobs = page_dom.xpath('//div[@class="job_seen_beacon"]')
                all_jobs.extend(jobs)

            seen_links = set()
            for job in all_jobs:
                job_link = base_url + get_job_link(job)
                if job_link in seen_links:
                    continue  # Skip duplicates
                seen_links.add(job_link)

                job_title = get_job_title(job)
                company_name = get_company_name(job)
                company_location = get_company_location(job)
                salary = get_salary(job_link)
                job_desc = get_job_desc(job_link)  # Extract job description from the job link
 
                record = [job_link, job_keyword, location_keyword, job_title, company_name, company_location, salary, job_desc]

                print(record)
                
                theWriter.writerow(record)

# Closing the web browser
driver.quit()
%history